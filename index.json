[{"categories":["Go语言"],"content":"基于 go1.22.6 源码解析 channel 底层源码 1、内部数据结构 所在位置/usr/local/go/src/runtime/chan.go type hchan struct { qcount uint // 当前channel中存在元素的数量 dataqsiz uint // 当前channel环形队列的容量（buffered channel 的大小） buf unsafe.Pointer // 环形队列的指针（存储元素的地方） elemsize uint16 // channel中每个元素的大小 closed uint32 // 表示 channel 是否已关闭 elemtype *_type // channel中元素类型 sendx uint // 环形队列的发送索引 recvx uint // 环形队列的接收索引 recvq waitq // 等待接收的 goroutine 队列 sendq waitq // 等待发送的 goroutine 队列 // lock protects all fields in hchan, as well as several // fields in sudogs blocked on this channel. // // Do not change another G's status while holding this lock // (in particular, do not ready a G), as this can deadlock // with stack shrinking. lock mutex } type waitq struct { first *sudog // 队列头部 last *sudog // 队列尾部 } // sudog 是一个 伪g，表示等待列表中的 g，用于包装协程的节点 // 一个 g 可以出现在多个等待列表中，因此一个 g 可能有多个 sudog； // 并且很多gs可能在等待同一个同步对象，因此一个对象可能有很多sudog。 type sudog struct { // The following fields are protected by the hchan.lock of the // channel this sudog is blocking on. shrinkstack depends on // this for sudogs involved in channel ops. g *g // 协程 next *sudog // 队列中的下一个节点 prev *sudog // 队列中的上一个节点 elem unsafe.Pointer // data element (may point to stack) ... c *hchan // 标识与当前sudog交互的channel } 2、构造函数 func makechan(t *chantype, size int) *hchan { elem := t.Elem // compiler checks this but be safe. if elem.Size_ \u003e= 1\u003c\u003c16 { throw(\"makechan: invalid channel element type\") } if hchanSize%maxAlign != 0 || elem.Align_ \u003e maxAlign { throw(\"makechan: bad alignment\") } mem, overflow := math.MulUintptr(elem.Size_, uintptr(size)) if overflow || mem \u003e maxAlloc-hchanSize || size \u003c 0 { panic(plainError(\"makechan: size out of range\")) } // Hchan does not contain pointers interesting for GC when elements stored in buf do not contain pointers. // buf points into the same allocation, elemtype is persistent. // SudoG's are referenced from their owning thread so they can't be collected. // TODO(dvyukov,rlh): Rethink when collector can move allocated objects. var c *hchan switch { case mem == 0: // Queue or element size is zero. c = (*hchan)(mallocgc(hchanSize, nil, true)) // Race detector uses this location for synchronization. c.buf = c.raceaddr() case elem.PtrBytes == 0: // Elements do not contain pointers. // Allocate hchan and buf in one call. c = (*hchan)(mallocgc(hchanSize+mem, nil, true)) c.buf = add(unsafe.Pointer(c), hchanSize) default: // Elements contain pointers. c = new(hchan) c.buf = mallocgc(mem, elem, true) } c.elemsize = uint16(elem.Size_) c.elemtype = elem c.dataqsiz = uint(size) lockInit(\u0026c.lock, lockRankHchan) if debugChan { print(\"makechan: chan=\", c, \"; elemsize=\", elem.Size_, \"; dataqsiz=\", size, \"\\n\") } return c } 1）校验元素大小和内存对齐，检查元素的对齐需求是否超过通道支持的最大对齐（8 字节）。 2）校验内存需求是否超限 mem = elem.Size_ * uintptr(size)：计算环形队列总的内存需求；overflow：检查是否发生整数溢出 overflow：如果内存溢出，抛出异常。 mem \u003e maxAlloc-hchanSize：如果总内存需求超过可分配的最大值，抛出异常。 size \u003c 0：如果通道的容量是负数，抛出异常。 3）按照三种格式分配内存，三种方式都指定了分配的内存不包含指针，避免 gc 扫描这块内存： 1、无缓冲通道：仅分配 hchanSize 2、有缓冲的 struct 类型：同时分配 hchanSize 和环形缓冲区的内存，优化分配性能，并且调整 chan 的 buf 指向 mem 的起始位置；此时两者是连续的，在一块连续的内存中；其中 mallocgc 参数 nil 表示内存中不包含指针，GC 不需要扫描这块内存，如果缓冲区中存储的元素包含指针，则会传递元素类型信息（非 nil）。 3、有缓冲的指针类型：分别分配 hchanSize 和环形缓冲区的内存。两者无需连续，mallocgc 参数需要指定元素类型，比如这里的c.buf = mallocgc(mem, elem, true)。 4）对 channel 的其余字段进行初始化，包括元素类型大小、元素类型、容量以及锁的初始化。 ","date":"2026-02-03","objectID":"/channle/:0:0","tags":["Go"],"title":"Channel解析","uri":"/channle/"},{"categories":["Go语言"],"content":"2.1 内存对齐说明： maxAlign = 8 hchanSize = unsafe.Sizeof(hchan{}) + uintptr(-int(unsafe.Sizeof(hchan{}))\u0026(maxAlign-1)) if hchanSize%maxAlign != 0 || elem.Align_ \u003e maxAlign { throw(\"makechan: bad alignment\") } maxAlign = 8: 最大对齐值，通道的结构体需要对齐到 8 字节的边界。 这是 Go 中的通用内存对齐策略，针对 64 位架构。 unsafe.Sizeof(hchan{}): 计算 hchan 结构体的大小。 这部分是未对齐的大小。 对齐计算： uintptr(-int(unsafe.Sizeof(hchan{})) \u0026 (maxAlign-1))是用于对齐的位操作。 它确保 hchanSize是 maxAlign 的倍数。 假设unsafe.Sizeof(hchan{})=70，则uintptr(-int(unsafe.Sizeof(hchan{}))\u0026(maxAlign-1)) = -70 \u0026 7 -70 的二进制表示（以补码形式表示）： 原码（70）： 0000 0000 0000 0000 0000 0000 0100 0110 反码（取反）： 1111 1111 1111 1111 1111 1111 1011 1001 补码（+1）： 1111 1111 1111 1111 1111 1111 1011 1010（即 -70） 掩码（7 的二进制）： 0000 0000 0000 0000 0000 0000 0000 0111 按位与： 1111 1111 1111 1111 1111 1111 1011 1010 \u0026 0000 0000 0000 0000 0000 0000 0000 0111 --------------------------------------- 0000 0000 0000 0000 0000 0000 0000 0010 结果为 2，表示需要补齐 2 字节。对齐后的 hchanSize 是 72，是 8 的整数倍，符合对齐要求。 3、写操作 //go:nosplit func chansend1(c *hchan, elem unsafe.Pointer) { chansend(c, elem, true, getcallerpc()) } func chansend(c *hchan, ep unsafe.Pointer, block bool, callerpc uintptr) bool { // 异常case1:通道未初始化，往里写会被挂起，死锁 if c == nil { if !block { return false } gopark(nil, nil, waitReasonChanSendNilChan, traceBlockForever, 2) throw(\"unreachable\") } if debugChan { print(\"chansend: chan=\", c, \"\\n\") } if raceenabled { racereadpc(c.raceaddr(), callerpc, abi.FuncPCABIInternal(chansend)) } // Fast path: check for failed non-blocking operation without acquiring the lock. if !block \u0026\u0026 c.closed == 0 \u0026\u0026 full(c) { return false } var t0 int64 if blockprofilerate \u003e 0 { t0 = cputicks() } lock(\u0026c.lock) // 异常case2:往关闭了的通道中写，panic if c.closed != 0 { unlock(\u0026c.lock) panic(plainError(\"send on closed channel\")) } // 正常case1: 等待接收的 goroutine 队列不为空 if sg := c.recvq.dequeue(); sg != nil { // Found a waiting receiver. We pass the value we want to send // directly to the receiver, bypassing the channel buffer (if any). send(c, sg, ep, func() { unlock(\u0026c.lock) }, 3) return true } // 正常case2: 缓冲区还有空间，找到sendx所在位置将数据发送 if c.qcount \u003c c.dataqsiz { // Space is available in the channel buffer. Enqueue the element to send. qp := chanbuf(c, c.sendx) if raceenabled { racenotify(c, c.sendx, nil) } typedmemmove(c.elemtype, qp, ep) c.sendx++ if c.sendx == c.dataqsiz { c.sendx = 0 } c.qcount++ unlock(\u0026c.lock) return true } // 正常case3: 此时说明没有等待接收的g，缓冲区也没有空间（或无缓冲channel），非阻塞模式直接返回 if !block { unlock(\u0026c.lock) return false } // 正常case4: 进入阻塞模式，gopark让出协程的调度权 // Block on the channel. Some receiver will complete our operation for us. gp := getg() // 从线程的局部存储TLS中获取当前g的指针 mysg := acquireSudog() // 准备一个伪g，代表处于等待列表中的 g mysg.releasetime = 0 if t0 != 0 { mysg.releasetime = -1 } // No stack splits between assigning elem and enqueuing mysg // on gp.waiting where copystack can find it. mysg.elem = ep mysg.waitlink = nil mysg.g = gp mysg.isSelect = false mysg.c = c gp.waiting = mysg gp.param = nil c.sendq.enqueue(mysg) // Signal to anyone trying to shrink our stack that we're about // to park on a channel. The window between when this G's status // changes and when we set gp.activeStackChans is not safe for // stack shrinking. gp.parkingOnChan.Store(true) gopark(chanparkcommit, unsafe.Pointer(\u0026c.lock), waitReasonChanSend, traceBlockChanSend, 2) // Ensure the value being sent is kept alive until the // receiver copies it out. The sudog has a pointer to the // stack object, but sudogs aren't considered as roots of the // stack tracer. KeepAlive(ep) // 主要功能是确保一个变量在调用点之前不会被 GC 回收 // someone woke us up. if mysg != gp.waiting { throw(\"G waiting list is corrupted\") } gp.waiting = nil gp.activeStackChans = false closed := !mysg.success gp.param = nil if mysg.releasetime \u003e 0 { blockevent(mysg.releasetime-t0, 2) } mysg.c = nil releaseSudog(mysg) if closed { if c.closed == 0 { throw(\"chansend: spurious wakeup\") } panic(plainError(\"send on closed channel\")) } return true } 阻塞发送： ","date":"2026-02-03","objectID":"/channle/:1:0","tags":["Go"],"title":"Channel解析","uri":"/channle/"},{"categories":["Go语言"],"content":"3.1 参数解释： c *hchan：指向通道的内部结构。 ep unsafe.Pointer：待发送的数据地址。 block bool：是否阻塞发送操作： true：阻塞操作直到数据发送成功。 false：非阻塞操作，如果无法发送立即返回。 callerpc uintptr：调用者的程序计数器（用于竞态检测）。 ","date":"2026-02-03","objectID":"/channle/:2:0","tags":["Go"],"title":"Channel解析","uri":"/channle/"},{"categories":["Go语言"],"content":"3.2 整体流程 1）对于未初始化的 chan 非阻塞模式直接返回 false。 阻塞模式调用 gopark 将当前 Goroutine 永久挂起（因为对 nil 通道的发送无法完成）。 2）如果启用了竞态检测，记录对通道的读取操作，用于后续分析。 3）非阻塞模式的快速路径 非阻塞模式： 如果通道未关闭（c.closed == 0）且缓冲区已满（full(c)），直接返回 false，表示发送失败。 优化点： 通过快速路径避免锁竞争，提高性能。 4）如果启用了性能分析（blockprofilerate \u003e 0），记录当前 CPU 时间戳（cputicks()），以便后续分析阻塞时间。 5）加锁 6）对已关闭的 channel 写入，会触发 panic 7）如果接收队列中存在等待接收的 Goroutine（recvq 非空），调用 send 将数据直接发送给接收 Goroutine，跳过通道缓冲区，直接完成发送操作。 8）如果缓冲区未满（c.qcount \u003c c.dataqsiz）： 将数据写入缓冲区（chanbuf(c, c.sendx) 指向缓冲区的发送位置） 更新发送索引（c.sendx），并维护环形队列 增加元素计数（c.qcount++） 解锁并返回 true，表示发送成功 9）非阻塞模式，且无法发送（接收队列为空，缓冲区已满），解锁并返回 false 10）接下来进入，阻塞模式，阻塞住。 将当前 Goroutine 挂起： 获取一个 sudog（代表当前 Goroutine 的调度数据结构），完成指针指向，建立 sudog、goroutine、channel 之间的指向关系 将当前 Goroutine 放入通道的发送队列（sendq），插入队尾。 调用 gopark 挂起 Goroutine，等待接收方唤醒。 11）倘若协程从 park 中被唤醒，则回收 sudog（sudog能被唤醒，其对应的元素必然已经被读协程取走）； 12）唤醒后检查，若通道关闭，则 panic ","date":"2026-02-03","objectID":"/channle/:3:0","tags":["Go"],"title":"Channel解析","uri":"/channle/"},{"categories":["Go语言"],"content":"3.2.1 send 函数 func send(c *hchan, sg *sudog, ep unsafe.Pointer, unlockf func(), skip int) { if raceenabled { if c.dataqsiz == 0 { racesync(c, sg) } else { // Pretend we go through the buffer, even though // we copy directly. Note that we need to increment // the head/tail locations only when raceenabled. racenotify(c, c.recvx, nil) racenotify(c, c.recvx, sg) c.recvx++ if c.recvx == c.dataqsiz { c.recvx = 0 } c.sendx = c.recvx // c.sendx = (c.sendx+1) % c.dataqsiz } } if sg.elem != nil { sendDirect(c.elemtype, sg, ep) sg.elem = nil } gp := sg.g unlockf() gp.param = unsafe.Pointer(sg) sg.success = true if sg.releasetime != 0 { sg.releasetime = cputicks() } goready(gp, skip+1) } 参数解释： c *hchan：通道。 sg *sudog：接收 Goroutine 的调度结构。 ep unsafe.Pointer：待发送数据的地址。 unlockf func()：释放锁的回调函数。 skip int：调用堆栈深度（用于调试）。 1）如果开启了竞态检测，记录缓冲区的读写操作 2）如果接收 Goroutine 的 elem 字段不为 nil，将数据直接复制到接收方 3）解锁后，调用 goready 唤醒接收 Goroutine 并标记操作成功 func sendDirect(t *_type, sg *sudog, src unsafe.Pointer) { // src is on our stack, dst is a slot on another stack. // Once we read sg.elem out of sg, it will no longer // be updated if the destination's stack gets copied (shrunk). // So make sure that no preemption points can happen between read \u0026 use. // 读取目标地址 dst := sg.elem // 写屏障，通知 GC 监视目标位置的数据写入，以防止垃圾回收期间的数据丢失或不一致 typeBitsBulkBarrier(t, uintptr(dst), uintptr(src), t.Size_) // No need for cgo write barrier checks because dst is always // Go memory. // 将数据从 src（发送方的栈位置）复制到 dst（接收方的栈位置）。 memmove(dst, src, t.Size_) } sendDirect 是 Go runtime 中用于实现无缓冲通道（unbuffered channel）或缓冲为空的通道中的直接发送操作的低级函数。这种直接发送操作的特点是发送方和接收方的 goroutine 必须协作配合，数据从一个 goroutine 的栈上直接移动到另一个 goroutine 的栈上。 在 goroutine 的栈之间直接操作内存，需要保证数据一致性和安全性，尤其是： 防止数据竞争： 避免因为 goroutine 栈被移动或被预占（如在垃圾回收或 goroutine 调度中）而导致的数据不一致。 保证垃圾回收（GC）的正确性： 因为 GC 假设只有当前 goroutine 会写入自己的栈，而此函数打破了这一假设，因此需要引入写屏障。 为什么需要特殊处理？ 在无缓冲通道或缓冲为空的情况下： 发送和接收 goroutine 需要直接交换数据，直接在两个栈之间移动数据比通过中间缓冲区（如堆分配）更高效，减少了内存分配和回收的开销。特别是对于无缓冲通道（直接点对点传递数据）来说，直接拷贝是最快的方式。 此操作涉及直接操作两个 goroutine 的栈，这种行为对垃圾回收器（GC）来说是特殊的，因为 GC 假设每个 goroutine 的栈只由该 goroutine 自己写入。 在 sendDirect 中，dst 是目标 goroutine 的栈上的地址。为了避免其他函数操作目标栈并导致地址不稳定，Go 运行时采取了一些严格的措施。这些措施包括防止栈的动态调整（如伸缩或移动）影响当前的操作。以下是可能导致栈伸缩的情况，以及为什么 sendDirect 是安全的： 栈的动态调整 Go 运行时支持 goroutine 栈的动态调整（增长或收缩），但这种调整通常在以下情况下发生： 栈不足：当 goroutine 分配的栈空间不足时，运行时会触发栈增长。 垃圾回收：垃圾回收期间可能会移动栈以压缩内存。 在 sendDirect 中，dst 的地址通过 sudog.elem 获取。如果目标栈在操作期间发生了动态调整，那么之前获取的 dst 地址可能会失效。 解决方案： sendDirect 的设计保证在获取 dst 地址后，确保操作不可中断（没有抢占点），直到数据拷贝完成。这避免了在栈调整过程中使用无效地址。 其他函数是否会操作 dst 的栈？ 通常，只有运行时或目标 goroutine 自己会操作目标栈。常见函数调用不会直接影响目标栈上的数据。 在 sendDirect 中，跨栈写操作是明确设计的一部分，并通过写屏障和内存管理屏障（如 typeBitsBulkBarrier）确保栈地址的正确性。 为何其他函数不会导致问题？ 隔离性：Go 的 goroutine 栈是独立的，普通函数调用只能操作调用 goroutine 的栈，无法直接操作其他 goroutine 的栈。 抢占安全性：Go 运行时在跨栈操作期间禁用了可能导致栈移动的抢占点，确保栈在操作期间保持稳定。 写屏障机制：写屏障（typeBitsBulkBarrier等）和内存管理逻辑确保即使垃圾回收发生，也可以正确追踪和更新目标地址。 ","date":"2026-02-03","objectID":"/channle/:3:1","tags":["Go"],"title":"Channel解析","uri":"/channle/"},{"categories":["Go语言"],"content":"3.2.2 KeepLive 函数说明 栈对象的存活 在 Goroutine 的栈上分配的对象（如局部变量），可能会在程序逻辑中引用到，但如果这些引用只存在于一些运行时数据结构（如 sudog）中，GC 可能会错误地将其回收。KeepAlive 明确告知 GC，这个变量必须存活到 KeepAlive 调用的位置。 在通道的发送逻辑中，ep 是一个指向待发送数据的指针。如果这个指针指向的是一个栈上的局部变量，GC 可能会在发送完成前回收这个变量。sudog 不被视为 GC 的根对象（root），这是因为sudog用于临时存储信息（如当前等待的通道）。虽然它包含了指向 ep 的指针，然而，GC 不会扫描 sudog，因此不会认为 ep 是活跃的对象。 需要手动通过 KeepAlive 确保 ep 的生命周期。 4、读操作 两种从 channel 中接收数据的方式，都是会去调用 chanrecv 函数 i \u003c- ch i, ok \u003c- ch // 带通道判断 //go:nosplit func chanrecv1(c *hchan, elem unsafe.Pointer) { chanrecv(c, elem, true) } //go:nosplit func chanrecv2(c *hchan, elem unsafe.Pointer) (received bool) { _, received = chanrecv(c, elem, true) return } // chanrecv receives on channel c and writes the received data to ep. // ep may be nil, in which case received data is ignored. // If block == false and no elements are available, returns (false, false). // Otherwise, if c is closed, zeros *ep and returns (true, false). // Otherwise, fills in *ep with an element and returns (true, true). // A non-nil ep must point to the heap or the caller's stack. func chanrecv(c *hchan, ep unsafe.Pointer, block bool) (selected, received bool) { if debugChan { print(\"chanrecv: chan=\", c, \"\\n\") } if c == nil { if !block { return } gopark(nil, nil, waitReasonChanReceiveNilChan, traceBlockForever, 2) throw(\"unreachable\") } // Fast path: check for failed non-blocking operation without acquiring the lock. if !block \u0026\u0026 empty(c) { // After observing that the channel is not ready for receiving, we observe whether the // channel is closed. // // Reordering of these checks could lead to incorrect behavior when racing with a close. // For example, if the channel was open and not empty, was closed, and then drained, // reordered reads could incorrectly indicate \"open and empty\". To prevent reordering, // we use atomic loads for both checks, and rely on emptying and closing to happen in // separate critical sections under the same lock. This assumption fails when closing // an unbuffered channel with a blocked send, but that is an error condition anyway. if atomic.Load(\u0026c.closed) == 0 { // Because a channel cannot be reopened, the later observation of the channel // being not closed implies that it was also not closed at the moment of the // first observation. We behave as if we observed the channel at that moment // and report that the receive cannot proceed. return } // The channel is irreversibly closed. Re-check whether the channel has any pending data // to receive, which could have arrived between the empty and closed checks above. // Sequential consistency is also required here, when racing with such a send. // 再次检查是否为空 // 有场景：可能因为在判断通道关闭的同时有另一个g往通道发送了数据，如果此时没有检查直接返回就会漏掉通道中的数据 if empty(c) { // The channel is irreversibly closed and empty.不可逆转的关闭且为空 if raceenabled { raceacquire(c.raceaddr()) } if ep != nil { typedmemclr(c.elemtype, ep) } return true, false } } var t0 int64 if blockprofilerate \u003e 0 { t0 = cputicks() } lock(\u0026c.lock) if c.closed != 0 { if c.qcount == 0 { if raceenabled { raceacquire(c.raceaddr()) } unlock(\u0026c.lock) if ep != nil { typedmemclr(c.elemtype, ep) } return true, false } // The channel has been closed, but the channel's buffer have data. } else { // Just found waiting sender with not closed. if sg := c.sendq.dequeue(); sg != nil { // Found a waiting sender. If buffer is size 0, receive value // directly from sender. Otherwise, receive from head of queue // and add sender's value to the tail of the queue (both map to // the same buffer slot because the queue is full). recv(c, sg, ep, func() { unlock(\u0026c.lock) }, 3) return true, true } } if c.qcount \u003e 0 { // Receive directly from queue qp := chanbuf(c, c.recvx) if raceenabled { racenotify(c, c.recvx, nil) } if ep != nil { typedmemmove(c.elemtype, ep, qp) } typedmemclr(c.elemtype, qp) c.recvx++ if c.recvx == c.dataqsiz { c.recvx = 0 } c.qcount-- unlock(\u0026c.lock) return true, true } if !block { unlock(\u0026c.lock) return false, false } // no sender available: block on this channel. gp := getg() mysg := acqu","date":"2026-02-03","objectID":"/channle/:3:2","tags":["Go"],"title":"Channel解析","uri":"/channle/"},{"categories":["Go语言"],"content":"4.1 参数解释 c *hchan: 指向目标 channel 的指针。 ep unsafe.Pointer: 接收数据的地址指针。如果为 nil，表示忽略接收到的数据。 block bool: 表示是否允许阻塞。如果为 false，表示非阻塞模式。v \u003c- ch和v, ok \u003c- ch两种都是阻塞的，对于使用select语句从channel中接收数据时，调用的是runtime.selectnbrecv，selectnbrecv表示非阻塞接收。selectnbrecv的实现也是调用的runtime.chanrecv，只是传参的时候block=false。因此这个block参数主要是区分是否是select receive。 返回值： selected bool: 表示当前通道的接收操作是否被成功选择执行。在多路选择场景下，用于标识当前通道是否是被选中执行的通道操作。 received bool: 是否成功接收到值。当通道已关闭且无数据可接收时，返回 false。 ","date":"2026-02-03","objectID":"/channle/:4:0","tags":["Go"],"title":"Channel解析","uri":"/channle/"},{"categories":["Go语言"],"content":"4.2 整体流程 1）如果通道为空，非阻塞模式直接返回，阻塞模式会调用 gopark 出让调度权，陷入死锁。fatal error: all goroutines are asleep - deadlock! 2）进入非阻塞模式的快速路径判断，先判断 chan 是否为空，为空进入以下逻辑处理 加原子锁判断是否已关闭，若未关闭，此时 chan 状态是未关闭且为空，返回 （false，false） 加原子锁判断是否已关闭，若已关闭，则再次判断是否为空，若为空，此时状态是已关闭且为空，则返回 (true, false)，并将接收缓冲区清零。不为空则继续下面的逻辑，表示缓存区有数据，尽管 chan 已经被关闭，仍然要处理 3）如果启用了性能分析，记录当前时间戳 t0，用于后续计算阻塞时间。 4）加锁 5）如果 channel 已关闭且缓冲区为空： 解锁。 如果 ep 不为空，将目标内存清零。 返回 (true, false)。 6）未关闭，检查是否有等待发送的 goroutine，有则调用 recv 函数从发送方直接接收数据，解锁并返回 (true, true)。 7）如果缓冲区有数据： 计算出下一个接收位置 qp。 如果 ep 不为 nil，将数据移动到接收地址。 清空缓冲区位置，更新索引和计数。 解锁并返回 (true, true)。 8）如果是非阻塞模式，但此时没有数据，也没有等待的发送者，直接返回 (false, false) 9）阻塞模式将当前 goroutine 加入等待队列： 当前 goroutine获取一个 sudog。 设置 sudog 的字段（例如目标地址、关联的 goroutine 等）。 将 sudog 加入 recvq 队列。 调用 gopark 出让调度权 10）被唤醒后：检查 sudog 是否仍然与当前 goroutine 关联。从 sudog 中提取结果并释放资源，返回接收结果 ","date":"2026-02-03","objectID":"/channle/:5:0","tags":["Go"],"title":"Channel解析","uri":"/channle/"},{"categories":["Go语言"],"content":"4.2.1 recv 函数 func recv(c *hchan, sg *sudog, ep unsafe.Pointer, unlockf func(), skip int) { if c.dataqsiz == 0 { if raceenabled { racesync(c, sg) } if ep != nil { // copy data from sender recvDirect(c.elemtype, sg, ep) } } else { // Queue is full. Take the item at the // head of the queue. Make the sender enqueue // its item at the tail of the queue. Since the // queue is full, those are both the same slot. qp := chanbuf(c, c.recvx) if raceenabled { racenotify(c, c.recvx, nil) racenotify(c, c.recvx, sg) } // copy data from queue to receiver if ep != nil { typedmemmove(c.elemtype, ep, qp) } // copy data from sender to queue typedmemmove(c.elemtype, qp, sg.elem) c.recvx++ if c.recvx == c.dataqsiz { c.recvx = 0 } c.sendx = c.recvx // c.sendx = (c.sendx+1) % c.dataqsiz } sg.elem = nil gp := sg.g unlockf() gp.param = unsafe.Pointer(sg) sg.success = true if sg.releasetime != 0 { sg.releasetime = cputicks() } goready(gp, skip+1) } 参数： c *hchan: 目标 channel，调用该函数时，c 已经加锁 sg *sudog: 表示等待发送的 goroutine 的 sudog，sg 已经从 channel 的发送队列中移除（dequeue） ep unsafe.Pointer: 表示接收方的目标内存地址，如果为 nil，则表示忽略接收到的值 unlockf func(): 解锁函数，用于在操作完成后释放锁 skip int: 跟踪调用栈的深度，用于调度相关的调用（如 goready） 整体流程： 调用该函数的时候，从等待发送队列中取出一个 g，说明如果是带缓冲带的 chan，此时缓冲区一定是满的 1）如果c.dataqsiz == 0，属于无缓冲 channle，通过 recvDirect 跨栈直接发送数据，无需操作缓冲区 2）如果c.dataqsiz \u003e 0：表示这是一个**有缓冲 channel，**需要从缓冲区中获取值，并将发送方的数据写入缓冲区 chanbuf：计算缓冲区中当前接收位置的地址 racenotify：通知竞态检测器，当前接收和发送位置即将发生操作 ep 不为空，从缓冲区当前接收位置 qp 复制数据到接收方地址 ep 从发送方的 sg.elem 中复制数据到缓冲区当前接收位置 qp 更新接收索引 recvx 和发送索引 sendx，如果到达缓冲区末尾，循环回到开头，由于此时缓冲区已满，recvx 和 sendx 指向同一个位置 3）sg.elem = nil 清除 sg 中的元素引用，避免内存泄漏，获取发送方 goroutine 的指针 gp，调用解锁函数 unlockf，释放对 channel 的锁 4）将 sg 设置为发送方 goroutine 的参数（param），标记 sg.success = true，调用 goready 唤醒发送者，将发送 goroutine 标记为可运行状态。 总结：recv 的主要逻辑 无缓冲 channel：数据直接通过recvDirect从发送方传递给接收方。直接内存拷贝（typedmemmove）优化数据传递 有缓冲 channel：从缓冲区中获取接收方的数据，将发送方的数据放入缓冲区的尾部，更新缓冲区的状态索引 唤醒发送方：将发送方 goroutine 标记为可运行状态，使其恢复执行 func recvDirect(t *_type, sg *sudog, dst unsafe.Pointer) { // dst is on our stack or the heap, src is on another stack. // The channel is locked, so src will not move during this // operation. src := sg.elem typeBitsBulkBarrier(t, uintptr(dst), uintptr(src), t.Size_) memmove(dst, src, t.Size_) } recvDirect 同 sendDirect，直接垮栈交换数据，怎么实现的？ 1、sg.elem 是发送方 goroutine 的栈上的一个地址，指向发送方准备好的数据。通道在运行时会将发送 goroutine 的数据位置存储到 sudog.elem 中。chan 的加锁保证了发送方的 sudog.elem 不会在拷贝期间被修改或移动。 2、通过写屏障保证确保 GC 在扫描对象时不会遗漏或误判引用： 因为 src 和 dst 可能分别位于不同的栈或堆上，并且跨栈写入是特殊情况，运行时需要调用 typeBitsBulkBarrier 来通知 GC： 数据的来源（**src**）和目标（dst）。 数据的大小和类型（t.Size_）。 这可以确保在 GC 扫描时正确处理栈与堆之间的内存引用。 3、memmove是一个底层的内存拷贝函数，用于将指定大小的数据从 src 复制到 dst。这里直接将发送方的内存数据从 sg.elem（发送方栈上的数据）复制到接收方的存储位置 dst。 5、关闭 chan ","date":"2026-02-03","objectID":"/channle/:5:1","tags":["Go"],"title":"Channel解析","uri":"/channle/"},{"categories":["Go语言"],"content":"5.1 关键入口与执行路径 close(ch) func walkExpr(n ir.Node, init *ir.Nodes) ir.Node { .... case ir.OCLEAR: n := n.(*ir.UnaryExpr) return walkClear(n) case ir.OCLOSE: n := n.(*ir.UnaryExpr) return walkClose(n, init) case ir.OMAKECHAN: n := n.(*ir.MakeExpr) return walkMakeChan(n, init) case ir.OMAKEMAP: n := n.(*ir.MakeExpr) return walkMakeMap(n, init) case ir.OMAKESLICE: n := n.(*ir.MakeExpr) return walkMakeSlice(n, init) ... } 在编译器编译期间，节点类型为 OLCLOSE，在go/src/cmd/compile/internal/walk/expr.go中处理该节点的函数是go/src/cmd/compile/internal/walk/builtin.go 中的 walkClose 函数，可以看到会被编译器处理为调用运行时函数 closechan // walkClose walks an OCLOSE node. func walkClose(n *ir.UnaryExpr, init *ir.Nodes) ir.Node { // cannot use chanfn - closechan takes any, not chan any fn := typecheck.LookupRuntime(\"closechan\", n.X.Type()) return mkcall1(fn, nil, init, n.X) } ","date":"2026-02-03","objectID":"/channle/:6:0","tags":["Go"],"title":"Channel解析","uri":"/channle/"},{"categories":["Go语言"],"content":"5.2 流程 func closechan(c *hchan) { if c == nil { panic(plainError(\"close of nil channel\")) } lock(\u0026c.lock) if c.closed != 0 { unlock(\u0026c.lock) panic(plainError(\"close of closed channel\")) } if raceenabled { callerpc := getcallerpc() racewritepc(c.raceaddr(), callerpc, abi.FuncPCABIInternal(closechan)) racerelease(c.raceaddr()) } c.closed = 1 var glist gList // release all readers for { sg := c.recvq.dequeue() if sg == nil { break } if sg.elem != nil { typedmemclr(c.elemtype, sg.elem) sg.elem = nil } if sg.releasetime != 0 { sg.releasetime = cputicks() } gp := sg.g gp.param = unsafe.Pointer(sg) sg.success = false if raceenabled { raceacquireg(gp, c.raceaddr()) } glist.push(gp) } // release all writers (they will panic) for { sg := c.sendq.dequeue() if sg == nil { break } sg.elem = nil if sg.releasetime != 0 { sg.releasetime = cputicks() } gp := sg.g gp.param = unsafe.Pointer(sg) sg.success = false if raceenabled { raceacquireg(gp, c.raceaddr()) } glist.push(gp) } unlock(\u0026c.lock) // Ready all Gs now that we've dropped the channel lock. for !glist.empty() { gp := glist.pop() gp.schedlink = 0 goready(gp, 3) } } 1）关闭未被初始化的 chan，会触发 panic 2）加锁 3）判断是否已被关闭，若关闭已经被关闭的 chan，触发 panic，解锁 4）关闭 chan，c.closed 赋值为 1 5）将接收队列中的所有 g 使用 typedmemclr 将 elem 的内存区域清零，添加到 glist，后续通过 goready 唤醒 6）将发送队列中的所有 g 也添加到 glist 7）解锁 8）唤醒 glist 中的所有 g 6、其他补充知识 ","date":"2026-02-03","objectID":"/channle/:7:0","tags":["Go"],"title":"Channel解析","uri":"/channle/"},{"categories":["Go语言"],"content":"6.1 sudog type sudog struct { g *g next *sudog prev *sudog elem unsafe.Pointer acquiretime int64 releasetime int64 ticket uint32 isSelect bool success bool waiters uint16 parent *sudog waitlink *sudog waittail *sudog c *hchan } sudog 是 Go runtime 中的一个核心结构体，用于管理 goroutine 在 channel 操作中的等待状态。sudog 是 channel 的底层实现的一部分，通过它可以记录哪些 goroutines 正在等待操作，如何调度它们，以及相关的元信息。 以下是 sudog 结构体各字段的详细说明： g *g 含义：指向正在等待操作的 goroutine 的结构体。 作用：sudog 是与 goroutine 紧密关联的。通过这个字段可以找到具体的 goroutine，完成调度、唤醒等操作。 next *sudog 和 prev *sudog 含义：形成一个双向链表，next 指向下一个等待的 sudog，prev 指向上一个。 作用：维护等待队列，特别是在 channel 的发送和接收操作中，存储所有等待的 goroutines。 elem unsafe.Pointer 含义：指向需要被发送或接收的数据的地址。 作用：在 channel 操作中，用于保存发送数据或者接收数据的目标地址（通常是一个变量指针）。 场景：发送方会将数据存放到 elem 指向的地址中，接收方从 elem 指向的地址读取数据。 acquiretime int64 和 releasetime int64 含义：分别记录 sudog 被添加到等待队列的时间和从队列中移除的时间。 作用： acquiretime：记录 goroutine 开始等待时的时间戳（纳秒）。 releasetime：记录 goroutine 被唤醒时的时间戳。 用途：可以用来调试、性能分析或检测 goroutine 是否被长时间阻塞。 ticket uint32 含义：一个标识，用来排序或调度。 作用：可以帮助实现公平调度，特别是在竞争激烈的情况下，决定哪个 goroutine 优先被唤醒。 isSelect bool 含义：标记该 sudog 是否与 select 语句关联。 作用： 如果 goroutine 是通过 select 语句阻塞的，这个标志会被设置为 true。 特殊处理 select 的唤醒逻辑。 success bool 含义：记录当前操作是否成功。 作用：在某些情况下，用于判断发送或接收操作是否完成（例如与 select 配合时）。 waiters uint16 含义：表示当前 goroutine 关联的等待者数量。 作用：在调试和性能分析中可以用来检查 goroutine 是否有过多的依赖或等待者。 parent *sudog 含义：指向父级 sudog。 作用：构建一个层级结构，用于管理嵌套的等待关系。 场景：在某些复杂场景中，用来描述多层等待。 waitlink *sudog 和 waittail *sudog 含义： waitlink：链表中的下一个等待节点。 waittail：链表的尾部节点。 作用：维护一个链表，用于描述当前 sudog 的等待子集。 c *hchan 含义：指向与该 sudog 关联的 channel。 作用：标明该 sudog 当前在哪个 channel 上进行操作（发送或接收）。 场景：调度或唤醒时需要通过这个字段找到对应的 channel。 总体作用： •场景：sudog 的主要用途是管理 goroutine 在 channel 上的阻塞操作。无论是发送还是接收操作，如果 channel 满或空，goroutine 就会通过一个 sudog 结构体被挂起到 channel 的等待队列中。 •操作： • 如果是发送操作，数据会存储在 elem 指向的地址中。 • 如果是接收操作，数据会从 elem 指向的地址读取。 •恢复：当条件满足时（如 channel 有空间或有数据），runtime 会唤醒队列中的 sudog，对应的 goroutine 继续运行。 ","date":"2026-02-03","objectID":"/channle/:8:0","tags":["Go"],"title":"Channel解析","uri":"/channle/"},{"categories":["Go语言"],"content":"6.2 chanrecv 与 chansend 的快速路径判断 // chanrecv if !block \u0026\u0026 empty(c) { // 1、加锁的目的是：避免在读取channel状态的同时有另一个G正在关闭该通道，导致数据不一致 // 2、而加原子锁的原因是：1、相比锁操作，更轻量级，适合在快速路径中使用，尽量减少开销 // 2、通道关闭是单向的，只能从未关闭-\u003e关闭，且数据状态变更很少 if atomic.Load(\u0026c.closed) == 0 { return } // 再次检查是否为空 // 有场景：可能因为在判断通道关闭的同时有另一个g往通道发送了数据，如果此时没有检查直接返回就会漏掉通道中的数据 if empty(c) { // The channel is irreversibly closed and empty.不可逆转的关闭且为空 if raceenabled { raceacquire(c.raceaddr()) } if ep != nil { typedmemclr(c.elemtype, ep) } return true, false } } // chansend if !block \u0026\u0026 c.closed == 0 \u0026\u0026 full(c) { return false } chanrecv 和 chansend 的区别 通道的发送（chansend）和接收（chanrecv）逻辑中对关闭状态的处理逻辑不同： 接收（chanrecv）： 接收操作需要非常精准地判断通道的状态，因为接收需要知道： 通道是否已经关闭。 通道是否还有数据。 如果通道已经关闭且没有数据，接收操作会返回默认值。 因为关闭通道和接收数据可能是并发发生的，假设先判断通道是否为空，再判断是否关闭，如果顺序错乱，可能会错误地报告通道“未关闭但为空”，导致逻辑错误因此必须通过 原子操作（**atomic.Load**）来确保读取到的状态是准确的。 发送（chansend）： 发送操作只需要判断通道是否关闭。 如果通道关闭，发送会直接失败。 发送和关闭通道之间虽然也可能是并发的，但 Go 设计里，这种场景是更简单的，因为通道关闭是一个单向操作（只能从未关闭到关闭），不会反复变化。如果在判断时，c.closed 的值刚好从未关闭（0）变成了关闭（1），快速路径可能会错误地判断通道“未关闭”，进入后续逻辑，但没关系，因为后续逻辑会再次获取锁并检查通道状态。在这种设计中，快速路径的判断只是一种“优化”，并不是最终的结果。 因此，在发送场景中，对 c.closed 的判断不需要像接收那样那么严格。 ","date":"2026-02-03","objectID":"/channle/:9:0","tags":["Go"],"title":"Channel解析","uri":"/channle/"},{"categories":["Go语言"],"content":"6.3 阻塞模式与 非阻塞模式 func chansend(c *hchan, ep unsafe.Pointer, block bool, callerpc uintptr) bool {} func chanrecv(c *hchan, ep unsafe.Pointer, block bool) (selected, received bool) {} 非阻塞模式下，通过 block 参数设置为 false 实现，在函数处理过程中： （1） 所有需要使得当前 goroutine 被挂起和进入死锁的操作，在非阻塞模式下都会返回 false； （2）所有能立即完成读取/写入操作的条件下，非阻塞模式下会返回 true。 同样的，对于 select 关键字，在多路复用分支中，有 defalut 分支的情况下，会被编译器优化为调用运行时 selectnbsend 和 selectnbrecv 函数，而该函数底层仍然是调用 chansend 和 chanrecv，只不过是传递 block=false，表示非阻塞模式。具体可见 select 的底层实现。 // compiler implements // // select { // case c \u003c- v: // ... foo // default: // ... bar // } // // as // // if selectnbsend(c, v) { // ... foo // } else { // ... bar // } func selectnbsend(c *hchan, elem unsafe.Pointer) (selected bool) { return chansend(c, elem, false, getcallerpc()) } // compiler implements // // select { // case v, ok = \u003c-c: // ... foo // default: // ... bar // } // // as // // if selected, ok = selectnbrecv(\u0026v, c); selected { // ... foo // } else { // ... bar // } func selectnbrecv(elem unsafe.Pointer, c *hchan) (selected, received bool) { return chanrecv(c, elem, false) } ","date":"2026-02-03","objectID":"/channle/:10:0","tags":["Go"],"title":"Channel解析","uri":"/channle/"},{"categories":["Go语言"],"content":"坐好扶好马上揭晓、内容劲爆有点重要 1、使用方式 主要用于追踪 goroutine，达到控制它们的作用，一个goroutine开启后，要么等他自己结束，要么等外界通知它结束。对于后台监控类的goroutine，如果没有外界通知它结束的话会一直运行下去，可以通过chan+select控制，但是这种方式无法处理goroutine中开goroutine的情况，可能会有多个嵌套的goroutine，不可能写那么多的case \u003c-stop。 使用context比较好，context.Background() 返回一个空的Context，这个空的Context一般用于整个Context树的根节点。然后我们使用context.WithCancel(parent)函数，创建一个可取消的子Context，然后当作参数传给goroutine使用，这样就可以使用这个子Context跟踪这个goroutine。 在goroutine中，使用select调用\u003c-ctx.Done()判断是否要结束，如果接受到值的话，就可以返回结束goroutine了；如果接收不到，就会继续进行监控。 那么是如何发送结束指令的呢？这就是示例中的 cancel 函数啦，它是我们调用context.WithCancel(parent)函数生成子Context的时候返回的，第二个返回值就是这个取消函数，它是CancelFunc类型的。我们调用它就可以发出取消指令，然后我们的监控 goroutine 就会收到信号，就会返回结束。 // 使用chan+select func main() { stop := make(chan bool) go func() { for { select { case \u003c-stop: fmt.Println(\"监控退出，停止了...\") return default: fmt.Println(\"goroutine监控中...\") time.Sleep(2 * time.Second) } } }() time.Sleep(10 * time.Second) fmt.Println(\"可以了，通知监控停止\") stop\u003c- true //为了检测监控过是否停止，如果没有监控输出，就表示停止了 time.Sleep(5 * time.Second) } // 使用context func main() { ctx, cancel := context.WithCancel(context.Background()) go watch(ctx,\"【监控1】\") go watch(ctx,\"【监控2】\") go watch(ctx,\"【监控3】\") time.Sleep(10 * time.Second) fmt.Println(\"可以了，通知监控停止\") cancel() //为了检测监控过是否停止，如果没有监控输出，就表示停止了 time.Sleep(5 * time.Second) } func watch(ctx context.Context, name string) { for { select { case \u003c-ctx.Done(): fmt.Println(name,\"监控退出，停止了...\") return default: fmt.Println(name,\"goroutine监控中...\") time.Sleep(2 * time.Second) } } } 2、底层实现（go 1.22） ","date":"2026-02-03","objectID":"/context/:0:0","tags":["Go"],"title":"Context","uri":"/context/"},{"categories":["Go语言"],"content":"2.1 Context 内部数据结构： type Context interface { Deadline() (deadline time.Time, ok bool) Done() \u003c-chan struct{} Err() error Value(key any) any } （1）Deadline：返回 context 的过期时间； （2）Done：返回 context 中的 channel； （3）Err：返回错误； // 当context 取消时，由 [Context.Err] 返回 Canceled var Canceled = errors.New(\"context canceled\") // 当context 超时，由 [Context.Err] 返回 DeadlineExceeded var DeadlineExceeded error = deadlineExceededError{} type deadlineExceededError struct{} // 实现 Error 接口 func (deadlineExceededError) Error() string { return \"context deadline exceeded\" } （4）Value：返回 context 中的对应 key 的值. ","date":"2026-02-03","objectID":"/context/:1:0","tags":["Go"],"title":"Context","uri":"/context/"},{"categories":["Go语言"],"content":"2.2 emptyCtx 空实现 func (emptyCtx) Deadline() (deadline time.Time, ok bool) { return } // 返回一个 nil，用户无论往 nil 中写入或者读取数据，均会陷入阻塞 func (emptyCtx) Done() \u003c-chan struct{} { return nil } func (emptyCtx) Err() error { return nil } func (emptyCtx) Value(key any) any { return nil } type backgroundCtx struct{ emptyCtx } type todoCtx struct{ emptyCtx } func Background() Context { return backgroundCtx{} } func TODO() Context { return todoCtx{} } 可以看到平常使用的 context.Background()实际是返回一个 context 接口的空实现。TODO() 也是如此。 ","date":"2026-02-03","objectID":"/context/:2:0","tags":["Go"],"title":"Context","uri":"/context/"},{"categories":["Go语言"],"content":"2.3 cancelCtx 实现 平常我们大多数都是使用 带 cancel 的 context 比如： ctx, cancel := context.WithCancel(context.Background()) 在调用内部其实是调用了 withCancel 函数返回一个 cancelCtx 结构体对象，而该对象是实现了 Context 接口的。 type CancelFunc func() func WithCancel(parent Context) (ctx Context, cancel CancelFunc) { c := withCancel(parent) return c, func() { c.cancel(true, Canceled, nil) } } func withCancel(parent Context) *cancelCtx { if parent == nil { panic(\"cannot create context from nil parent\") } c := \u0026cancelCtx{} c.propagateCancel(parent, c) return c } 在看 c.propagateCancel(parent, c)方法实现之前，先看看 cancelCtx这个结构体的内部数据结构是怎样的。 ","date":"2026-02-03","objectID":"/context/:3:0","tags":["Go"],"title":"Context","uri":"/context/"},{"categories":["Go语言"],"content":"2.3.1 内部数据接口定义 CancelCtx 可以被取消。取消时，它还会取消其所有子 Context。 type cancelCtx struct { Context mu sync.Mutex // protects following fields done atomic.Value // of chan struct{}, created lazily, closed by first cancel call children map[canceler]struct{} // set to nil by the first cancel call err error // set to non-nil by the first cancel call cause error // set to non-nil by the first cancel call } // cancelCtx同时实现了canceler接口 // A canceler is a context type that can be canceled directly. The // implementations are *cancelCtx and *timerCtx. type canceler interface { cancel(removeFromParent bool, err,cause error) Done() \u003c-chan struct{} } 1、内置了一个 Context，作为其父 Context，cancelCtx 必然为某个 Context 的子 Context 2、mutex，加锁获取并发场景的资源 3、children：一个 map，指向 cancelCtx 的所有子 context；key 是一个实现了 canceler 接口的实现类。该 map 保存着从该 context 延伸下去的子节点，每一个子节点都是一个实现了canceler 接口的实现类，再看看 canceler 接口的实现方法，这些是暴露给子 context 的方法，父context 仅仅要求 子context 实现这两个方法即可，子context 有具体的其他方法可由子context 做对应的增加。保持职责内聚，仅关注自己需要关注的部分即可，也是为了避免暴露过多的方法给父context，造成错误调用。 继续看 c.propagateCancel(parent, c)方法实现，用来将父 context 的取消事件传递给子 context，其中参数中的 child 是一个 canceler 接口，cancelCtx 也实现了该接口，因此可以作为参数传递进来。 在函数内部 // 函数的作用是确保当父上下文（parent）被取消时，子上下文（child）也能被相应地取消 func (c *cancelCtx) propagateCancel(parent Context, child canceler) { c.Context = parent // 给 父Context 赋值 done := parent.Done() // 1、父 Context永远不会取消，比如emptyCtx，直接返回 if done == nil { return // parent is never canceled } // 2、select 监听 done 通道是否有值，有值代表父Context已经被取消， // 此时作为该 父 Context的子Context也应该被取消，同时，该子Context的后代Context也应该被取消 // 也就是调用 child.cancel 方法进行取消，从这里能够看到 Context是有父子关系的 select { case \u003c-done: // parent is already canceled child.cancel(false, parent.Err(), Cause(parent)) return default: } // 3、判断 parent 最内层的cancelCtx是否是go自己内部实现的cancelCtx // 如果是，则把 child 添加到该 cancelCtx 的 child 中 if p, ok := parentCancelCtx(parent); ok { // parent is a *cancelCtx, or derives from one. p.mu.Lock() // 访问共享资源 p.children map结构，加锁 if p.err != nil { // parent has already been canceled child.cancel(false, p.err, p.cause) } else { if p.children == nil { // 懒加载 p.children = make(map[canceler]struct{}) } p.children[child] = struct{}{} } p.mu.Unlock() return } // 4、是否实现了AfterFunc方法 if a, ok := parent.(afterFuncer); ok { // parent implements an AfterFunc method. c.mu.Lock() stop := a.AfterFunc(func() { child.cancel(false, parent.Err(), Cause(parent)) }) c.Context = stopCtx{ Context: parent, stop: stop, } c.mu.Unlock() return } //5、如果不是go自己内部实现的cancelCtx则另外起一个协程，一直监听父context是否关闭，如果关闭则关闭child goroutines.Add(1) go func() { select { case \u003c-parent.Done(): child.cancel(false, parent.Err(), Cause(parent)) case \u003c-child.Done(): } }() } // cancelCtxKey 是 cancelCtx 返回自身的键 var cancelCtxKey int // 该方法主要是根据cancelCtxKey查找parent最底层的cancelCtx func parentCancelCtx(parent Context) (*cancelCtx, bool) { done := parent.Done() if done == closedchan || done == nil { return nil, false } p, ok := parent.Value(\u0026cancelCtxKey).(*cancelCtx) if !ok { return nil, false } pdone, _ := p.done.Load().(chan struct{}) if pdone != done { return nil, false } return p, true } ","date":"2026-02-03","objectID":"/context/:3:1","tags":["Go"],"title":"Context","uri":"/context/"},{"categories":["Go语言"],"content":"2.3.2 cancelCtx 实现 Context 接口的四个方法： // 1、key 等于 特定值cancelCtxKey，返回cancelCtx自身的指针 // 2、否则，根据父context来取值 func (c *cancelCtx) Value(key any) any { if key == \u0026cancelCtxKey { return c } return value(c.Context, key) } func value(c Context, key any) any { for { switch ctx := c.(type) { case *valueCtx: if key == ctx.key { return ctx.val } c = ctx.Context case *cancelCtx: if key == \u0026cancelCtxKey { return c } c = ctx.Context case withoutCancelCtx: if key == \u0026cancelCtxKey { // This implements Cause(ctx) == nil // when ctx is created using WithoutCancel. return nil } c = ctx.c case *timerCtx: if key == \u0026cancelCtxKey { return \u0026ctx.cancelCtx } c = ctx.Context case backgroundCtx, todoCtx: return nil default: return c.Value(key) } } } //（1）基于 atomic 包，读取 cancelCtx 中的 chan；倘若已存在，则直接返回； //（2）加锁后，再次检查 chan 是否存在，若存在则返回；（double check） //（3）不存在，则初始化 chan 存储到 aotmic.Value 当中，并返回（懒加载机制） func (c *cancelCtx) Done() \u003c-chan struct{} { d := c.done.Load() // done是 atomic 包 if d != nil { return d.(chan struct{}) } c.mu.Lock() defer c.mu.Unlock() d = c.done.Load() if d == nil { d = make(chan struct{}) c.done.Store(d) } return d.(chan struct{}) } // 加锁赋值给err，解锁返回 func (c *cancelCtx) Err() error { c.mu.Lock() err := c.err c.mu.Unlock() return err } Deadline方法： 直接复用父Context的Deadline方法。 ","date":"2026-02-03","objectID":"/context/:3:2","tags":["Go"],"title":"Context","uri":"/context/"},{"categories":["Go语言"],"content":"2.3.3 cancelCtx.cancel方法 // cancel closes c.done, cancels each of c's children, and, if // removeFromParent is true, removes c from its parent's children. // cancel sets c.cause to cause if this is the first time c is canceled. func (c *cancelCtx) cancel(removeFromParent bool, err, cause error) { if err == nil { panic(\"context: internal error: missing cancel error\") } // 兼容老版本，以前版本不需要传 cause，仅传err if cause == nil { cause = err } c.mu.Lock() if c.err != nil { c.mu.Unlock() return // already canceled } c.err = err c.cause = cause d, _ := c.done.Load().(chan struct{}) if d == nil { c.done.Store(closedchan) } else { close(d) } for child := range c.children { // NOTE: acquiring the child's lock while holding parent's lock. child.cancel(false, err, cause) } c.children = nil c.mu.Unlock() if removeFromParent { removeChild(c.Context, c) } } // removeChild removes a context from its parent. func removeChild(parent Context, child canceler) { if s, ok := parent.(stopCtx); ok { s.stop() return } p, ok := parentCancelCtx(parent) if !ok { return } p.mu.Lock() if p.children != nil { delete(p.children, child) } p.mu.Unlock() } （1）该方法有三个入参，第一个 removeFromParent 是一个 bool 值，表示当前 context 是否需要从父 context 的 children map 集合中删除；第二个 err 是 cancel 后需要展示的错误信息；第三个 cause 同 err，在第一次cancel 调用时被设置。 （2）校验传进来的 err 是否为空，若为空则 panic （3）加锁 （4）判断 cancelCtx 自身的 err 是否为 nil，若不为 nil，表示已经被 cancel 了，解锁返回 （5）给 cancelCtx 的 err 和 cause 赋值，处理 cancelCtx 的 channel，若 channel 此前未初始化，则直接注入一个 closedChan，否则关闭该 channel； （6）遍历当前 cancelCtx 的 children map 集合，依次将 children context 都进行 cancel；将当前 cancelCtx 的 children 字段置为 nil；这里是一个递归调用，在获取 child contxet 锁的同时也持有 parent context 的锁 （7）解锁 （8）如果 removeFromParent 为 true，调用 removeChild 方法，先判断是否是 cancelCtx，不是则直接返回（因为只有 cancelCtx 才有 children map）；把当前 cancelCtx 从其父 context 的 children map 中 delete；操作共享资源加锁 ","date":"2026-02-03","objectID":"/context/:3:3","tags":["Go"],"title":"Context","uri":"/context/"},{"categories":["Go语言"],"content":"2.4 timerCtx： // 内嵌了一个 cancelCtx，用来实现 Done 和 Err 方法，并通过停止计时器然后委托给cancelCtx.cancel 来实现取消 type timerCtx struct { cancelCtx timer *time.Timer // Under cancelCtx.mu. deadline time.Time } timerCtx 在 cancelCtx 基础上又做了一层封装，除了继承 cancelCtx 的能力之外，新增了一个 time.Timer 用于定时终止 context；另外新增了一个 deadline 字段用于判断 timerCtx 的过期时间. 来看初始化一个带截止时间的 Context ： // 创建 timeout, c := context.WithTimeout(context.Background(), 10*time.Minute) func WithTimeout(parent Context, timeout time.Duration) (Context, CancelFunc) { return WithDeadline(parent, time.Now().Add(timeout)) } func WithDeadline(parent Context, d time.Time) (Context, CancelFunc) { return WithDeadlineCause(parent, d, nil) } func WithDeadlineCause(parent Context, d time.Time, cause error) (Context, CancelFunc) { if parent == nil { panic(\"cannot create context from nil parent\") } if cur, ok := parent.Deadline(); ok \u0026\u0026 cur.Before(d) { // The current deadline is already sooner than the new one. return WithCancel(parent) } c := \u0026timerCtx{ deadline: d, } c.cancelCtx.propagateCancel(parent, c) dur := time.Until(d) if dur \u003c= 0 { c.cancel(true, DeadlineExceeded, cause) // deadline has already passed return c, func() { c.cancel(false, Canceled, nil) } } c.mu.Lock() defer c.mu.Unlock() if c.err == nil { c.timer = time.AfterFunc(dur, func() { c.cancel(true, DeadlineExceeded, cause) }) } return c, func() { c.cancel(true, Canceled, nil) } } 1）parent == nil 报错 2）如果父 Context 的截止时间早于当前 Context，返回一个 cancelCtx 3）初始化 timeCtx，并将 parent 的 cancel 事件同步到子 Context 4）判断是否到达截止时间，若是，则取消该 timeCtx 5）加锁 6）启动 time.Timer，并规定好到达截止时间时，调用 cancel 方法取消 Context，并返回超时错误 7）解锁，返回 timerCtx 和一个可取消的 func func (c *timerCtx) Deadline() (deadline time.Time, ok bool) { return c.deadline, true } // 借助cancelCtx的cancel方法，然后取消定时器 func (c *timerCtx) cancel(removeFromParent bool, err, cause error) { c.cancelCtx.cancel(false, err, cause) if removeFromParent { // Remove this timerCtx from its parent cancelCtx's children. removeChild(c.cancelCtx.Context, c) } c.mu.Lock() if c.timer != nil { c.timer.Stop() c.timer = nil } c.mu.Unlock() } ","date":"2026-02-03","objectID":"/context/:4:0","tags":["Go"],"title":"Context","uri":"/context/"},{"categories":["Go语言"],"content":"2.5 valueCtx valueCtx 携带一个 key-value 键值对，其他的 context 接口方法都委托给内嵌的 Context type valueCtx struct { Context key, val any } type ContextKey string // 定义键 const key ContextKey = \"userID\" // 创建一个带值的上下文 ctxWithValue := context.WithValue(context.Background(), key, \"12345\") WithValue： func WithValue(parent Context, key, val any) Context { if parent == nil { panic(\"cannot create context from nil parent\") } if key == nil { panic(\"nil key\") } if !reflectlite.TypeOf(key).Comparable() { panic(\"key is not comparable\") } return \u0026valueCtx{parent, key, val} } valueCtx.Value 方法： func (c *valueCtx) Value(key any) any { if c.key == key { return c.val } return value(c.Context, key) } func value(c Context, key any) any { for { switch ctx := c.(type) { case *valueCtx: if key == ctx.key { return ctx.val } c = ctx.Context case *cancelCtx: if key == \u0026cancelCtxKey { return c } c = ctx.Context case withoutCancelCtx: if key == \u0026cancelCtxKey { // This implements Cause(ctx) == nil // when ctx is created using WithoutCancel. return nil } c = ctx.c case *timerCtx: if key == \u0026cancelCtxKey { return \u0026ctx.cancelCtx } c = ctx.Context case backgroundCtx, todoCtx: return nil default: return c.Value(key) } } } 1）如果当前 valueCtx 的 key 等于传入的 key，返回其 value 2）不等于，则向上往父 Context 查找 valueCtx 使用注意： context.WithValue 的值是不可变的，如果需要修改值，必须创建新的上下文。 不建议在上下文中存储过大的数据，如结构体或切片。 不要滥用 WithValue**，过多的键值对可能导致上下文混乱。不支持基于 k 的去重，相同 k 可能重复存在，并基于起点的不同，返回不同的 v。仅用于在请求范围内传递元数据，例如：用户 ID、请求跟踪 ID 等 使用自定义类型作为键，并且是可比较的 避免使用基础类型（如 string 或 int）作为键，容易引发冲突。 使用自定义类型（例如 type ContextKey string）确保键在上下文中是唯一的，避免与其他包的键冲突。 基于 k 寻找 v 的过程是线性的，时间复杂度 O(N) ","date":"2026-02-03","objectID":"/context/:5:0","tags":["Go"],"title":"Context","uri":"/context/"},{"categories":["数据库"],"content":"学习 go 语言通用的结构化查询流程框架 – 标准库 database/sql 一、抽象接口定义 标准库 database/sql 定义了 go 语言通用的结构化查询流程框架，其对接的数据库类型是灵活多变的，如 mysql、sqlite、oracle 等. 因此在 database/sql 中，与具体数据库交互的细节内容统一托付给一个抽象的数据库驱动模块，在其中声明好一套适用于各类关系型数据库的统一规范，将各个关键节点定义成抽象的 interface，由具体类型的数据库完成数据库驱动模块的实现，然后将其注入到 database/sql 的大框架之中. database/sql 关于数据库驱动模块下各核心 interface 主要包括： Connector：抽象的数据库连接器，需要具备创建数据库连接以及返回从属的数据库驱动的能力 Driver：抽象的数据库驱动，具备创建数据库连接的能力 Conn：抽象的数据库连接，具备预处理 sql 以及开启事务的能力 Tx：抽象的事务，具备提交和回滚的能力 Statement：抽象的请求预处理状态. 具备实际执行 sql 并返回执行结果的能力 Result/Row：抽象的 sql 执行结果 ","date":"2026-02-03","objectID":"/database_sql/:0:0","tags":["GORM","Go"],"title":"database/sql","uri":"/database_sql/"},{"categories":["数据库"],"content":"1、 抽象接口 由具体的数据库类型提供具体的实现版本 // 抽象的数据库连接器 type Connector interface { // 获取一个数据库连接 Connect(context.Context) (Conn, error) // 获取数据库驱动 Driver() Driver } // 抽象的数据库连接 type Conn interface { // 预处理 sql Prepare(query string) (Stmt, error) // 关闭连接 Close() error // 开启事务 Begin() (Tx, error) } // 抽象的请求预处理状态接口 type Stmt interface { // 关闭 Close() error // 返回 sql 中存在的可变参数数量 NumInput() int // 执行操作类型的 sql Exec(args []Value) (Result, error) // 执行查询类型的 sql Query(args []Value) (Rows, error) } // 抽象的执行结果接口 type Result interface { // 最后一笔插入数据的主键 LastInsertId() (int64, error) // 操作影响的行数 RowsAffected() (int64, error) } type Rows interface { // 返回所有列名 Columns() []string // 关闭 rows 迭代器 Close() error // 遍历 Next(dest []Value) error } // 抽象的事务接口 type Tx interface { // 提交事务 Commit() error // 回滚事务 Rollback() error } // 抽象的数据库驱动 type Driver interface { // 开启一个新的数据库连接 Open(name string) (Conn, error) } ","date":"2026-02-03","objectID":"/database_sql/:1:0","tags":["GORM","Go"],"title":"database/sql","uri":"/database_sql/"},{"categories":["数据库"],"content":"2、创建数据库实例 先看下 db 实例的结构体字段： type DB struct { // Total time waited for new connections.记录等待新连接的总时长,主要用于统计和调优连接池的性能 waitDuration atomic.Int64 connector driver.Connector // 用于创建新连接的 Connector 对象,通常由数据库驱动实现 numClosed atomic.Uint64 // 用于记录数据库连接池中已经关闭的连接数。Stmt.openStmt 会检查这个字段，以清理关闭的连接。 mu sync.Mutex freeConn []*driverConn // 连接池中的空闲连接 ordered by returnedAt oldest to newest connRequests connRequestSet // 存储等待的连接请求 numOpen int // 表示当前打开（包括正在打开和已打开）的连接数 openerCh chan struct{} // 通知连接池需要打开新连接,是一个不携带数据的通道，通常用于实现信号通知机制 closed bool // db 实例是否已关闭 dep map[finalCloser]depSet // 存储依赖于 finalCloser 类型的对象，管理对象之间的依赖关系，确保在关闭时正确清理资源 lastPut map[*driverConn]string // 最后一次将连接放入连接池时的堆栈信息；debug only maxIdleCount int // 设置空闲连接池的最大连接数，为零表示使用默认值，如果为负数则表示不允许空闲连接 maxOpen int // 允许的最大打开连接数，\u003c= 0 表示不限制最大连接数 maxLifetime time.Duration // 连接的最大生命周期 maxIdleTime time.Duration // 连接在空闲状态下的最长存活时间 cleanerCh chan struct{} waitCount int64 // 请求连接时的等待次数 maxIdleClosed int64 // 因空闲连接超出最大数目而关闭的连接数 maxIdleTimeClosed int64 // 因空闲时间过长而被关闭的连接数 maxLifetimeClosed int64 // 因连接生命周期到期而被关闭的连接数 stop func() // 关闭连接池时调用，用于清理相关资源并停止连接创建操作 } 以一个例子开始对 database/sql 的源码解读 import ( \"context\" \"database/sql\" \"testing\" // 注册 mysql 数据库驱动 _ \"github.com/go-sql-driver/mysql\" ) type user struct { UserID int64 } func Test_sql(t *testing.T) { // 创建 db 实例 db, err := sql.Open(\"mysql\", \"username:passpord@(ip:port)/database\") if err != nil { t.Error(err) return } // 执行 sql ctx := context.Background() row := db.QueryRowContext(ctx, \"SELECT user_id FROM user WHERE id = 1\") if row.Err() != nil { t.Error(err) return } // 解析结果 var u user if err = row.Scan(\u0026u.UserID); err != nil { t.Error(err) return } t.Log(u.UserID) } 从 sql.Open() 函数开始，sql.Open 不会立即建立与数据库的连接。连接是在实际需要时创建的，例如首次执行查询/或者调用 DB.Ping var driversMu sync.RWMutex // 全局变量，读写锁 //go:linkname drivers var drivers = make(map[string]driver.Driver) func Open(driverName, dataSourceName string) (*DB, error) { driversMu.RLock() driveri, ok := drivers[driverName] driversMu.RUnlock() if !ok { return nil, fmt.Errorf(\"sql: unknown driver %q (forgotten import?)\", driverName) } if driverCtx, ok := driveri.(driver.DriverContext); ok { connector, err := driverCtx.OpenConnector(dataSourceName) if err != nil { return nil, err } return OpenDB(connector), nil } return OpenDB(dsnConnector{dsn: dataSourceName, driver: driveri}), nil } 具体执行流程： 1、加读写锁访问全局资源 drivers map，从中取出对应的驱动器实现类，各个不同的数据库有其对应的实现类，通过 Register 函数注册到 database/sql 的基础框架中。 2、若对应的驱动实现了连接器对象 Connector ，则获取并创建 db 实例 3、调用 OpenDB 函数，开始创建 db 实例 Register 函数： func Register(name string, driver driver.Driver) { driversMu.Lock() defer driversMu.Unlock() if driver == nil { panic(\"sql: Register driver is nil\") } if _, dup := drivers[name]; dup { panic(\"sql: Register called twice for driver \" + name) } drivers[name] = driver } 其中 driver.DriverContext 也是一个接口类型，具体为: type DriverContext interface { // OpenConnector must parse the name in the same format that Driver.Open // parses the name parameter. OpenConnector(name string) (Connector, error) } 接着看 OpenDB 函数的实现： var connectionRequestQueueSize = 1000000 // OpenDB may just validate its arguments without creating a connection // to the database. To verify that the data source name is valid, call // [DB.Ping]. func OpenDB(c driver.Connector) *DB { ctx, cancel := context.WithCancel(context.Background()) db := \u0026DB{ connector: c, openerCh: make(chan struct{}, connectionRequestQueueSize), lastPut: make(map[*driverConn]string), stop: cancel, } go db.connectionOpener(ctx) return db } func (db *DB) connectionOpener(ctx context.Context) { for { select { case \u003c-ctx.Done(): return // 等待新的连接请求到达 openerCh 通道。 case \u003c-db.openerCh: db.openNewConnection(ctx) } } } 1、创建 DB 实例 connector: 实现了 driver.Connector 接口，用于创建数据库连接。 openerCh: 一个缓冲通道，作为连接请求的等待队列。容量由 connectionRequestQueueSize 定义，决定了最多可以同时等待的连接请求数 lastPut: 记录最后一次放回池中的连接，用于调试或监控。 connRequests: 用于管理连接请求，每个请求与唯一的 ID 关联。 stop: 通过 context.CancelFunc 控制 connectionOpener 的停止。 2、启动一个 connectionOpener 协程，注入 ctx，以此来控制该协程的生命周期 在 connectionOpener 协程中，通过 for sele","date":"2026-02-03","objectID":"/database_sql/:2:0","tags":["GORM","Go"],"title":"database/sql","uri":"/database_sql/"},{"categories":["数据库"],"content":"2.1 连接池的管理 2.1.1 连接的表达形式 一条连接是怎么表达的？数据库驱动连接通过如下结构包装了一层，添加了创建时间，锁保护机制等，它负责管理连接的生命周期，包括重置、关闭等操作。 // driverConn wraps a driver.Conn with a mutex, to // be held during all calls into the Conn. (including any calls onto // interfaces returned via that Conn, such as calls on Tx, Stmt, // Result, Rows) type driverConn struct { db *DB createdAt time.Time sync.Mutex // guards following ci driver.Conn needReset bool // The connection session should be reset before use if true. closed bool finalClosed bool // ci.Close has been called openStmt map[*driverStmt]bool // guarded by db.mu inUse bool returnedAt time.Time // Time the connection was created or returned. onPut []func() // code (with db.mu held) run when conn is next returned dbmuClosed bool // same as closed, but guarded by db.mu, for removeClosedStmtLocked } ci driver.Conn: 实际的数据库连接，满足 driver.Conn 接口 needReset bool: 标志：如果为 true，表示连接需要在使用前重置 closed bool: 标志：表示此连接是否已关闭 finalClosed bool: 标志：表示底层连接的 Close 方法是否已被调用 openStmt map[*driveStmt]bool: 当前连接上是否有打开的语句 inUse bool: 标志：该连接是否正在被使用 returnedAt time.Time: 连接被创建或返回的时间 onPut []func(): 当连接被返回时执行的代码（持有 db.mu 锁时） dbmuClosed bool: 与 closed 类似，但由 db.mu 锁保护，避免并发访问 2.1.2 put放入连接池 那么连接是怎么被放入连接池的？连接池是怎么管理的？ putConnDBLocked 函数主要用于管理数据库连接池的行为。它的主要作用是处理一个连接请求（connRequest），或者将一个数据库连接（driverConn）放入空闲连接池中。如果既没有满足连接请求，也没有将连接放入空闲池，它将返回 false。 // Satisfy a connRequest or put the driverConn in the idle pool and return true // or return false. // putConnDBLocked will satisfy a connRequest if there is one, or it will // return the *driverConn to the freeConn list if err == nil and the idle // connection limit will not be exceeded. // If err != nil, the value of dc is ignored. // If err == nil, then dc must not equal nil. // If a connRequest was fulfilled or the *driverConn was placed in the // freeConn list, then true is returned, otherwise false is returned. func (db *DB) putConnDBLocked(dc *driverConn, err error) bool { // 检查 db 是否已关闭 if db.closed { return false } // 检查是否超过 db 允许打开的最大连接数 if db.maxOpen \u003e 0 \u0026\u0026 db.numOpen \u003e db.maxOpen { return false } // 从连接请求队列中随机取一个连接请求，若成功取到则标记 dc 连接已被使用，并通过 chan 发送给连接请求等待方 if req, ok := db.connRequests.TakeRandom(); ok { if err == nil { dc.inUse = true } req \u003c- connRequest{ conn: dc, err: err, } return true } else if err == nil \u0026\u0026 !db.closed { if db.maxIdleConnsLocked() \u003e len(db.freeConn) { db.freeConn = append(db.freeConn, dc) db.startCleanerLocked() return true } db.maxIdleClosed++ } return false } // connRequestSet is a set of chan connRequest that's // optimized for: // // - adding an element // - removing an element (only by the caller who added it) // - taking (get + delete) a random element // // We previously used a map for this but the take of a random element // was expensive, making mapiters. This type avoids a map entirely // and just uses a slice. type connRequestSet struct { // s are the elements in the set. s []connRequestAndIndex } type connRequestAndIndex struct { // req is the element in the set. req chan connRequest // curIdx points to the current location of this element in // connRequestSet.s. It gets set to -1 upon removal. curIdx *int } 1）优先处理等待的连接请求，往通道中发送连接给等待者 2）如果没有等待的连接请求，并且 err == nil（连接有效）且数据库未关闭； 检查空闲连接池（db.freeConn）是否未满： 调用 db.maxIdleConnsLocked() 获取允许的最大空闲连接数。 如果空闲池未满，将 dc 放入 db.freeConn，并启动连接清理器（startCleanerLocked），以确保空闲连接在过期时被清理。 返回 true 表示连接已成功放入空闲池。 如果空闲池已满： 增加 db.maxIdleClosed 计数，表示因空闲池已满而被关闭的连接数量。 2.1.3 清理空闲连接 startCleanerLocked 定时清理空闲连接： // startCleanerLocked starts connectionCleaner if needed. func (db *DB) startCleanerLocked() { if (db.maxLifetime \u003e 0 || db.maxIdleTime \u003e 0) \u0026\u0026 db.numOpen \u003e 0 \u0026\u0026 db.cleanerCh == nil { db.cleanerCh = make(chan struct{}, 1) go db.connectionCleaner(db.shortestIdleTimeLocked()) } } func (db *DB) shortestIdleTimeLocked() time.Duration { if db.maxIdleTime \u003c= 0 { return db.maxLifetime } if db.maxLifetime \u003c= 0 { return db.maxIdleTime } return min(db.maxIdleTime, db.maxLifetime) } func (db *DB) connectionCleaner(d time.Duration) { const minInterval = time.S","date":"2026-02-03","objectID":"/database_sql/:2:1","tags":["GORM","Go"],"title":"database/sql","uri":"/database_sql/"},{"categories":["数据库"],"content":"3、执行请求流程 ","date":"2026-02-03","objectID":"/database_sql/:3:0","tags":["GORM","Go"],"title":"database/sql","uri":"/database_sql/"},{"categories":["数据库"],"content":"3.1、执行入口 func (db *DB) QueryRowContext(ctx context.Context, query string, args ...any) *Row { rows, err := db.QueryContext(ctx, query, args...) return \u0026Row{rows: rows, err: err} } func (db *DB) QueryContext(ctx context.Context, query string, args ...any) (*Rows, error) { var rows *Rows var err error err = db.retry(func(strategy connReuseStrategy) error { rows, err = db.query(ctx, query, args, strategy) return err }) return rows, err } // maxBadConnRetries is the number of maximum retries if the driver returns // driver.ErrBadConn to signal a broken connection before forcing a new // connection to be opened. const maxBadConnRetries = 2 const ( // alwaysNewConn forces a new connection to the database. alwaysNewConn connReuseStrategy = iota // cachedOrNewConn returns a cached connection, if available, else waits // for one to become available (if MaxOpenConns has been reached) or // creates a new database connection. cachedOrNewConn ) func (db *DB) retry(fn func(strategy connReuseStrategy) error) error { for i := int64(0); i \u003c maxBadConnRetries; i++ { err := fn(cachedOrNewConn) // retry if err is driver.ErrBadConn if err == nil || !errors.Is(err, driver.ErrBadConn) { return err } } return fn(alwaysNewConn) } 1、连接过期而导致发生偶发性的 ErrBadConn 错误，针对这种错误，可以采用重试的方式来提高请求的成功率。连续遇到两次 ErrBadConn 之后，会将策略调整为不采用连接池直接新建连接的方式，再兜底执行一次请求。 2、实际调用 db.query 方法进行查询 在 db.query 方法中根据不同策略获取数据库连接，调用 db.conn 方法，调用 db.queryDC 方法执行 sql 查询 func (db *DB) query(ctx context.Context, query string, args []any, strategy connReuseStrategy) (*Rows, error) { dc, err := db.conn(ctx, strategy) if err != nil { return nil, err } return db.queryDC(ctx, nil, dc, dc.releaseConn, query, args) } ","date":"2026-02-03","objectID":"/database_sql/:3:1","tags":["GORM","Go"],"title":"database/sql","uri":"/database_sql/"},{"categories":["数据库"],"content":"3.2 获取数据库连接 // conn returns a newly-opened or cached *driverConn. func (db *DB) conn(ctx context.Context, strategy connReuseStrategy) (*driverConn, error) { db.mu.Lock() if db.closed { db.mu.Unlock() return nil, errDBClosed } // Check if the context is expired. select { default: case \u003c-ctx.Done(): db.mu.Unlock() return nil, ctx.Err() } lifetime := db.maxLifetime // Prefer a free connection, if possible. last := len(db.freeConn) - 1 if strategy == cachedOrNewConn \u0026\u0026 last \u003e= 0 { // Reuse the lowest idle time connection so we can close // connections which remain idle as soon as possible. conn := db.freeConn[last] db.freeConn = db.freeConn[:last] conn.inUse = true if conn.expired(lifetime) { db.maxLifetimeClosed++ db.mu.Unlock() conn.Close() return nil, driver.ErrBadConn } db.mu.Unlock() // Reset the session if required. if err := conn.resetSession(ctx); errors.Is(err, driver.ErrBadConn) { conn.Close() return nil, err } return conn, nil } // Out of free connections or we were asked not to use one. If we're not // allowed to open any more connections, make a request and wait. if db.maxOpen \u003e 0 \u0026\u0026 db.numOpen \u003e= db.maxOpen { // Make the connRequest channel. It's buffered so that the // connectionOpener doesn't block while waiting for the req to be read. req := make(chan connRequest, 1) delHandle := db.connRequests.Add(req) db.waitCount++ db.mu.Unlock() waitStart := nowFunc() // Timeout the connection request with the context. select { case \u003c-ctx.Done(): // Remove the connection request and ensure no value has been sent // on it after removing. db.mu.Lock() deleted := db.connRequests.Delete(delHandle) db.mu.Unlock() db.waitDuration.Add(int64(time.Since(waitStart))) // If we failed to delete it, that means either the DB was closed or // something else grabbed it and is about to send on it. if !deleted { // TODO(bradfitz): rather than this best effort select, we // should probably start a goroutine to read from req. This best // effort select existed before the change to check 'deleted'. // But if we know for sure it wasn't deleted and a sender is // outstanding, we should probably block on req (in a new // goroutine) to get the connection back. select { default: case ret, ok := \u003c-req: if ok \u0026\u0026 ret.conn != nil { db.putConn(ret.conn, ret.err, false) } } } return nil, ctx.Err() case ret, ok := \u003c-req: db.waitDuration.Add(int64(time.Since(waitStart))) if !ok { return nil, errDBClosed } // Only check if the connection is expired if the strategy is cachedOrNewConns. // If we require a new connection, just re-use the connection without looking // at the expiry time. If it is expired, it will be checked when it is placed // back into the connection pool. // This prioritizes giving a valid connection to a client over the exact connection // lifetime, which could expire exactly after this point anyway. if strategy == cachedOrNewConn \u0026\u0026 ret.err == nil \u0026\u0026 ret.conn.expired(lifetime) { db.mu.Lock() db.maxLifetimeClosed++ db.mu.Unlock() ret.conn.Close() return nil, driver.ErrBadConn } if ret.conn == nil { return nil, ret.err } // Reset the session if required. if err := ret.conn.resetSession(ctx); errors.Is(err, driver.ErrBadConn) { ret.conn.Close() return nil, err } return ret.conn, ret.err } } db.numOpen++ // optimistically db.mu.Unlock() ci, err := db.connector.Connect(ctx) if err != nil { db.mu.Lock() db.numOpen-- // correct for earlier optimism db.maybeOpenNewConnections() db.mu.Unlock() return nil, err } db.mu.Lock() dc := \u0026driverConn{ db: db, createdAt: nowFunc(), returnedAt: nowFunc(), ci: ci, inUse: true, } db.addDepLocked(dc, dc) db.mu.Unlock() return dc, nil } 大体流程： 1、若采用从连接池中复用连接的策略并且连接有空闲连接，则优先获取连接，获取成功则直接返回 2、如果没有空闲连接，或者策略要求新连接，则判断是否超过最大打开连接数 3、超过了，会将当前协程挂起，建立对应的 channel 添加到 connRequests 中，等待有连接释放时被唤醒 4、未超过则调用其实现类的 connector 创建新连接 ","date":"2026-02-03","objectID":"/database_sql/:3:2","tags":["GORM","Go"],"title":"database/sql","uri":"/database_sql/"},{"categories":["Go语言"],"content":"基于 go1.22.6 版本解析 func 函数底层实现 ","date":"2026-02-03","objectID":"/func/:0:0","tags":["Go"],"title":"Func函数底层实现","uri":"/func/"},{"categories":["Go语言"],"content":"一、函数执行用例 package main func main() { var r1, r2, r3, r4, r5, r6, r7, r8, r9, r10, r11 int64 = 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11 a, b := A(r1, r2, r3, r4, r5, r6, r7, r8, r9, r10, r11) c := a + b print(c) } func A(p1, p2, p3, p4, p5, p6, p7, p8, p9, p10, p11 int64) (int64, int64) { return p1 + p2 + p3 + p4 + p5 + p6 + p7, p2 + p4 + p6 + p7 + p8 + p9 + p10 + p11 } 执行命令打印 main.go 的汇编代码： GOOS=linux GOARCH=amd64 go tool compile -S -N -l main.go ","date":"2026-02-03","objectID":"/func/:1:0","tags":["Go"],"title":"Func函数底层实现","uri":"/func/"},{"categories":["Go语言"],"content":"二、汇编代码分析 分析汇编代码 main.main： \"\".main STEXT size=362 args=0x0 locals=0xe0 funcid=0x0 0x0000 00000 (main.go:3) TEXT \"\".main(SB), ABIInternal, $224-0 #main函数定义, $224-0：224表示将分配的main函数的栈帧大小；0指定了调用方传入的参数，由于main是最上层函数，这里没有入参 0x0000 00000 (main.go:3) LEAQ -96(SP), R12 0x0005 00005 (main.go:3) CMPQ R12, 16(R14) 0x0009 00009 (main.go:3) PCDATA $0, $-2 0x0009 00009 (main.go:3) JLS 349 0x000f 00015 (main.go:3) PCDATA $0, $-1 0x000f 00015 (main.go:3) SUBQ $224, SP # 为main函数栈帧分配了224字节的空间，注意此时的SP寄存器指向，会往下移动224个字节 0x0016 00022 (main.go:3) MOVQ BP, 216(SP) # BP寄存器存放的是main函数caller的基址，movq这条指令是将main函数caller的基址入栈 0x001e 00030 (main.go:3) LEAQ 216(SP), BP # 将main函数的基址存放到到BP寄存器 0x0026 00038 (main.go:3) FUNCDATA $0, gclocals·33cdeccccebe80329f1fdbee7f5874cb(SB) 0x0026 00038 (main.go:3) FUNCDATA $1, gclocals·33cdeccccebe80329f1fdbee7f5874cb(SB) 0x0026 00038 (main.go:4) MOVQ $1, \"\".r1+168(SP) # main函数局部变量r1入栈 0x0032 00050 (main.go:4) MOVQ $2, \"\".r2+144(SP) # main函数局部变量r2入栈 0x003e 00062 (main.go:4) MOVQ $3, \"\".r3+136(SP) # main函数局部变量r3入栈 0x004a 00074 (main.go:4) MOVQ $4, \"\".r4+128(SP) # main函数局部变量r4入栈 0x0056 00086 (main.go:4) MOVQ $5, \"\".r5+120(SP) # main函数局部变量r5入栈 0x005f 00095 (main.go:4) MOVQ $6, \"\".r6+112(SP) # main函数局部变量r6入栈 0x0068 00104 (main.go:4) MOVQ $7, \"\".r7+104(SP) # main函数局部变量r7入栈 0x0071 00113 (main.go:4) MOVQ $8, \"\".r8+96(SP) # main函数局部变量r8入栈 0x007a 00122 (main.go:4) MOVQ $9, \"\".r9+88(SP) # main函数局部变量r9入栈 0x0083 00131 (main.go:4) MOVQ $10, \"\".r10+160(SP) # main函数局部变量r10入栈 0x008f 00143 (main.go:4) MOVQ $11, \"\".r11+152(SP) # main函数局部变量r11入栈 0x009b 00155 (main.go:5) MOVQ \"\".r2+144(SP), BX # 将局部变量r2传给寄存器BX 0x00a3 00163 (main.go:5) MOVQ \"\".r3+136(SP), CX # 将局部变量r3传给寄存器CX 0x00ab 00171 (main.go:5) MOVQ \"\".r4+128(SP), DI # 将局部变量r4传给寄存器DI 0x00b3 00179 (main.go:5) MOVQ \"\".r5+120(SP), SI # 将局部变量r5传给寄存器SI 0x00b8 00184 (main.go:5) MOVQ \"\".r6+112(SP), R8 # 将局部变量r6传给寄存器R8 0x00bd 00189 (main.go:5) MOVQ \"\".r7+104(SP), R9 # 将局部变量r7传给寄存器R9 0x00c2 00194 (main.go:5) MOVQ \"\".r8+96(SP), R10 # 将局部变量r8传给寄存器R10 0x00c7 00199 (main.go:5) MOVQ \"\".r9+88(SP), R11 # 将局部变量r9传给寄存器R11 0x00cc 00204 (main.go:5) MOVQ \"\".r10+160(SP), DX # 将局部变量r10传给寄存器DX 0x00d4 00212 (main.go:5) MOVQ \"\".r1+168(SP), AX # 将局部变量r1传给寄存器DX 0x00dc 00220 (main.go:5) MOVQ DX, (SP) # 将寄存器DX保存的r10传给SP指向的栈顶 0x00e0 00224 (main.go:5) MOVQ $11, 8(SP) # 将变量r11传给SP+8 0x00e9 00233 (main.go:5) PCDATA $1, $0 0x00e9 00233 (main.go:5) CALL \"\".A(SB) # 调用 A 函数 0x00ee 00238 (main.go:5) MOVQ AX, \"\"..autotmp_14+208(SP) # 将寄存器AX存的函数A的第一个返回值a赋值给SP+208 0x00f6 00246 (main.go:5) MOVQ BX, \"\"..autotmp_15+200(SP) # 将寄存器BX存的函数A的第二个返回值b赋值给SP+200 0x00fe 00254 (main.go:5) MOVQ \"\"..autotmp_14+208(SP), DX # 将SP+208保存的A函数第一个返回值a传给寄存器DX 0x0106 00262 (main.go:5) MOVQ DX, \"\".a+192(SP) # 将A函数第一个返回值a通过寄存器DX入栈到SP+192 0x010e 00270 (main.go:5) MOVQ \"\"..autotmp_15+200(SP), DX # 将SP+200保存的A函数第二个返回值b传给寄存器DX 0x0116 00278 (main.go:5) MOVQ DX, \"\".b+184(SP) # 将第二个返回值b通过寄存器DX入栈到SP+184 0x011e 00286 (main.go:6) MOVQ \"\".a+192(SP), DX # 将返回值a传给DX寄存器 0x0126 00294 (main.go:6) ADDQ \"\".b+184(SP), DX # 将a+b赋值给DX寄存器 0x012e 00302 (main.go:6) MOVQ DX, \"\".c+176(SP) # 将DX寄存器的值入栈到SP+176 0x0136 00310 (main.go:7) CALL runtime.printlock(SB) 0x013b 00315 (main.go:7) MOVQ \"\".c+176(SP), AX # 将SP+176存储的入参c赋值给AX 0x0143 00323 (main.go:7) CALL runtime.printint(SB) # 调用打印函数打印c 0x0148 00328 (main.go:7) CALL runtime.printunlock(SB) 0x014d 00333 (main.go:8) MOVQ 216(SP), BP 0x0155 00341 (main.go:8) ADDQ $224, SP 0x015c 00348 (main.go:8) RET main函数调用A函数的参数个数为11个，其中前 9 个参数分别是通过寄存器 AX、BX、CX、DI、SI、R8、R9、R10、R11传递，后面两个通过栈顶的SP，SP+8地址传递。 下面再看函数 A 的汇编代码： \"\".A STEXT nosplit size=175 args=0x58 locals=0x18 funcid=0x0 0x0000 00000 (main.go:10) TEXT \"\".A(SB), NOSPLIT|ABIInternal, $24-88 0x0000 00000 (main.go:10) SUBQ $24, SP # 为A函数栈帧分配了24字节的空间 0x0004 00004 (main.go:10) MOVQ BP, 16(SP) 0x0009 00009 (main.go:10) LEAQ 16(SP), BP 0x000e 00014 (main.go:10) FUNCDATA $0, gclocals·33cdeccccebe80329f1fdbee7f5874cb(SB) 0x000e 00014 (main.go:10) FUNCDATA $1, gclocals·33cdeccccebe80329f1fdbee7f5874cb(SB)","date":"2026-02-03","objectID":"/func/:2:0","tags":["Go"],"title":"Func函数底层实现","uri":"/func/"},{"categories":["Go语言"],"content":"从底层源码角度看 golang 的 GMP 调度模型 ","date":"2026-02-03","objectID":"/gmp/:0:0","tags":["Go"],"title":"GMP调度模型","uri":"/gmp/"},{"categories":["Go语言"],"content":"1、基础概念 ","date":"2026-02-03","objectID":"/gmp/:1:0","tags":["Go"],"title":"GMP调度模型","uri":"/gmp/"},{"categories":["Go语言"],"content":"1.1 进程、线程、协程 进程是由一段静态代码经过编译为二进制可执行程序被加载到内存中经 CPU 按顺序执行的一个程序。 线程是进程中的一个执行流程，多个线程之间共享代码段、数据段、打开的文件等资源，但每个线程各自都有一套独立的寄存器和栈，以此来保证其控制流的独立性 协程是用户态线程，是用户程序对对线程概念的二次封装，和线程为多对一关系，在逻辑意义上属于更细粒度的调度单元，其调度过程由用户态闭环完成，无需内核介入 go 语言中的协程是 goroutine，能够与 M、P动态结合，体积只有几 KB，栈空间支持动态扩缩。 ","date":"2026-02-03","objectID":"/gmp/:1:1","tags":["Go"],"title":"GMP调度模型","uri":"/gmp/"},{"categories":["Go语言"],"content":"1.2 GMP架构 G goroutine，是 golang 中对协程的抽象；有自己的运行栈、生命周期状态、以及执行的任务函数（用户通过 go func 指定）；需要绑定在 m 上执行。 M machine，是 golang 中对线程的抽象；跟 p 结合，它的运行目标始终在 g0 和 g 之间，当运行 g0 时执行的是 m 的调度流程，负责寻找合适的“任务”，也就是 g；当运行 g 时，执行的是 m 获取到的”任务“，也就是用户通过 go func 启动的 goroutine，不断交替运行任务：寻找任务（执行g0）；执行任务（执行g） m 维护在一个 m 列表中，操作系统分配给当前 go 程序的线程数，m 的数目可以配置，空闲的 m 会回收或睡眠，有 m 阻塞时，会创建新的 m，因此其数目是动态的。 P processor，是 golang 中的调度器，m 需要与 p 绑定后，才会进入到 gmp 调度模式当中；因此 p 的数量决定了 g 最大并行数量（通过 GOMAXPROCS 进行设定），p 是 g 的存储容器，其自带一个本地 g 队列，承载着等待被调度的 g p 维护在一个 p 列表中，最大数目由 GOMAXPROCS 决定，可通过环境变量或 runtime 设置 如果将 GMP 看作一个任务管理系统：G 是任务，M 是运行任务的 work，P 是任务系统的中枢，跟 m 绑定开始寻找并运行 g，同时还是存储任务的容器 承载 G 的容器分为两部分： p 的本地队列 lrq（local run queue）：这是每个 p 私有的 g 队列，通常由 p 自行访问，并发竞争情况较少，因此设计为无锁化结构，通过 CAS（compare-and-swap）操作访问 全局队列 grq（global run queue）：是全局调度模块 schedt 中的全局共享 g 队列，作为当某个 lrq 不满足条件时的备用容器，因为不同的 m 都可能访问 grq，因此并发竞争比较激烈，访问前需要加全局锁 将 g 放入容器和取出容器的流程设计： put g：当某个 g 中通过 go func(){…} 操作创建子 g 时，会先尝试将子 g 添加到当前所在 p 的 lrq 中（无锁化）；如果 lrq 满了，则会将 g 追加到 grq 中（全局锁）。 get g：gmp 调度流程中，m 和 p 结合后，运行的 g0 会不断寻找合适的 g 用于执行，此时会采取“负载均衡”的思路，遵循如下实施步骤： 优先从当前 p 的 lrq 中获取 g（无锁化-CAS）,在 g0 每经过 61 次调度循环后，下一次在处理 lrq 前优先处理一次 grq，避免因 lrq 过于忙碌而致使 grq 陷入饥荒状态 从全局的 grq 中获取 g（全局锁） 取 io 就绪的 g（netpoll 机制） 从其他 p 的 lrq 中窃取 g（无锁化-CAS） 全局唯一的一个 m0，就是进程的主线程，拉起 main 函数这个 groutine。 每个 m 都有一个 g0，g0 仅负责调度 g 到 m 上运行，完成一个 g 到另一个 g 的切换。 M 的自旋状态：创建新的 G 时，运行的 G 会尝试唤醒其他空闲的 M 绑定 P 去执行，如果 G2 唤醒了M2，M2 绑定了一个 P2，会先运行 M2 的 G0，这时 M2 没有从 P2 的本地队列中找到 G，会进入自旋状态（spinning），自旋状态的 M2 会尝试从全局空闲线程队列里面获取 G，放到 P2 本地队列去执行，获取的数量满足公式：n = min(len(globrunqsize)/GOMAXPROCS + 1, len(localrunsize/2))，含义是每个P应该从全局队列承担的 G 数量，为了提高效率，不能太多，要给其他 P 留点； G 发生系统调用时：如果 G 发生系统调度进入阻塞，其所在的 M 也会阻塞，因为会进入内核状态等待系统资源，和 M 绑定的 P 会寻找空闲的 M 执行，这是为了提高效率，不能让 P 本地队列的 G 因所在 M 进入阻塞状态而无法执行；需要说明的是，如果是 M 上的 G 进入 Channel 阻塞，则该 M 不会一起进入阻塞，因为 Channel 数据传输涉及内存拷贝，不涉及系统资源等待； G 退出系统调用时：如果刚才进入系统调用的 G2 解除了阻塞，其所在的 M1 会寻找 P 去执行，优先找原来的 P，发现没有找到，则其上的 G2 会进入全局队列，等其他 M 获取执行，M1 进入空闲队列； ","date":"2026-02-03","objectID":"/gmp/:1:2","tags":["Go"],"title":"GMP调度模型","uri":"/gmp/"},{"categories":["Go语言"],"content":"2、go程序入口 查找程序的执行入口： https://zhuanlan.zhihu.com/p/654331129 一个 go 程序真正的入口在 runtime包的 src/runtime/rt0_linux_amd64.s (对 AMD64 架构上的 Linux，其他架构在不同的文件下) TEXT _rt0_amd64_linux(SB),NOSPLIT,$-8 JMP _rt0_amd64(SB) 跳转到了 src/runtime/asm_amd64.s 包的 _rt0_amd64 函数： // _rt0_amd64 is common startup code for most amd64 systems when using // internal linking. This is the entry point for the program from the // kernel for an ordinary -buildmode=exe program. The stack holds the // number of arguments and the C-style argv. TEXT _rt0_amd64(SB),NOSPLIT,$-8 MOVQ 0(SP), DI // argc LEAQ 8(SP), SI // argv JMP runtime·rt0_go(SB) 在 asm_amd64.s 中有 runtime·rt0_go 函数的实现： TEXT runtime·rt0_go(SB),NOSPLIT|NOFRAME|TOPFRAME,$0 // copy arguments forward on an even stack MOVQ DI, AX // argc，保存命令行参数 MOVQ SI, BX // argv ...... // create istack out of the given (operating system) stack. // _cgo_init may update stackguard. // 创建初始 goroutine（g0）的栈结构，设置栈边界 MOVQ $runtime·g0(SB), DI LEAQ (-64*1024)(SP), BX MOVQ BX, g_stackguard0(DI) MOVQ BX, g_stackguard1(DI) MOVQ BX, (g_stack+stack_lo)(DI) MOVQ SP, (g_stack+stack_hi)(DI) ...... ok: // 建立 goroutine（g0）和 machine（m0）的双向关联，g0 是初始 goroutine，m0 是初始 OS 线程，使用操作系统提 // 供的初始线程栈（通常是主线程栈） // set the per-goroutine and per-mach \"registers\" get_tls(BX) LEAQ runtime·g0(SB), CX MOVQ CX, g(BX) LEAQ runtime·m0(SB), AX // save m-\u003eg0 = g0 MOVQ CX, m_g0(AX) // save m0 to g0-\u003em MOVQ AX, g_m(CX) ....... // 解析命令行参数，操作系统相关初始化，调度器初始化 MOVL 24(SP), AX // copy argc MOVL AX, 0(SP) MOVQ 32(SP), AX // copy argv MOVQ AX, 8(SP) CALL runtime·args(SB) CALL runtime·osinit(SB) CALL runtime·schedinit(SB) // 创建一个新的 goroutine 来运行 runtime.main，runtime.main 最终会调用用户的 main.main() // create a new goroutine to start program MOVQ $runtime·mainPC(SB), AX // entry PUSHQ AX CALL runtime·newproc(SB) POPQ AX // start this M，启动调度 CALL runtime·mstart(SB) CALL runtime·abort(SB) // mstart should never return RET 这个汇编函数主要做了四件事： 建立 goroutine（g0）和 machine（m0）的双向关联，g0 是初始 goroutine，m0 是初始 OS 线程，它使用操作系统提供的初始线程栈（通常是主线程栈） 通过 runtime 中的 osinit、schedinit 等函数对 GMP 初始化。其中 osinit 主要是获取 CPU 数量，页大小等一些操作系统初始化工作 创建一个新的 goroutine 来运行 runtime.main，runtime.main 最终会调用用户的 main.main()，操作系统加载的时候只创建好了主线程，协程还是得用户态的 golang 自己管理，在这里 golang 创建出了自己的第一个协程 调用 runtime·mstart 开启调度 ","date":"2026-02-03","objectID":"/gmp/:2:0","tags":["Go"],"title":"GMP调度模型","uri":"/gmp/"},{"categories":["Go语言"],"content":"2.1 m0 和 g0 初始化过程 m0 和 g0 的初始化过程： // 建立 goroutine（g0）和 machine（m0）的双向关联，g0 是初始 goroutine，m0 是初始 OS 线程，使用操作系统提 // 供的初始线程栈（通常是主线程栈） // set the per-goroutine and per-mach \"registers\" get_tls(BX) LEAQ runtime·g0(SB), CX // 将全局变量 g0 的地址加载到 CX 寄存器，此时 CX 指向 g0 结构体 MOVQ CX, g(BX) //g(BX)宏展开为 0(r)(TLS*1)，即将 CX (g0 的地址)存储到 TLS 的第一个槽位 LEAQ runtime·m0(SB), AX // 将全局变量 m0 的地址加载到 AX 寄存器 // save m-\u003eg0 = g0 MOVQ CX, m_g0(AX) // save m0 to g0-\u003em MOVQ AX, g_m(CX) get_tls(BX):将 TLS 基地址加载到 BX 寄存器，TLS 代表当前线程的线程本地存储区域的基地址。 // get_tls(r) 在 go_tls.h 中定义 #ifdef GOARCH_amd64 #define get_tls(r) MOVQ TLS, r #define g(r) 0(r)(TLS*1) #endif 将全局变量 g0 的地址加载到 CX 寄存器，此时 CX 指向 g0 结构体 LEAQ runtime·g0(SB), CX g(BX)函数展开为 0(r)(TLS*1)，即将 CX (g0 的地址)存储到 TLS 的第一个槽位,从此刻起，getg() 函数就能工作了 MOVQ CX, g(BX) -------------------- // getg()的定义如下 // getg returns the pointer to the current g. // The compiler rewrites calls to this function into instructions // that fetch the g directly (from TLS or from the dedicated register). func getg() *g 将全局变量 m0 的地址加载到 AX 寄存器 LEAQ runtime·m0(SB), AX 此时的寄存器状态： BX = TLS 基地址 CX = \u0026g0 AX = \u0026m0 m-\u003eg0 = g0 MOVQ CX, m_g0(AX) g0-\u003em = m0 MOVQ AX, g_m(CX) 完成 m0 和 g0 的相互引用，通过 TLS，能够快速访问 GMP 的调度链 ┌─────────────────────────────────────┐ │ 当前线程（M） │ ├─────────────────────────────────────┤ │ TLS[0] → 当前 G │ │ ↓ │ │ g.m → 当前 M │ │ ↓ │ │ m.p → 当前 P │ │ ↓ │ │ p.runq → G队列 │ └─────────────────────────────────────┘ 使用 TLS 的好处： 1、通过 TLS，getg() 可以在几个 CPU 周期内完成，无需函数调用开销。 2、支持多线程每个操作系统线程（M）都有自己的 TLS：M1 的 TLS[0] 指向它当前运行的 G；M2 的 TLS[0] 指向它当前运行的 G。互不干扰，无需加锁。 3、架构无关的抽象：不同操作系统和 CPU 架构的 TLS 实现不同，通过 get_tls 宏和 g(r) 宏抽象了这些差异。 ","date":"2026-02-03","objectID":"/gmp/:2:1","tags":["Go"],"title":"GMP调度模型","uri":"/gmp/"},{"categories":["Go语言"],"content":"2.2 schedinit()调度系统初始化 schedinit() 函数对调度系统进行初始化，位于 runtime/proc.go 下，它负责初始化整个 Go 运行时环境 var ( m0 m // m0 作为全局变量静态分配 g0 g // m0 和 g0 是全局变量，在程序启动时就已经存在于数据段（.data 或 .bss 段） mcache0 *mcache raceprocctx0 uintptr raceFiniLock mutex ) // The bootstrap sequence is: 启动顺序 // // call osinit // call schedinit // make \u0026 queue new G // call runtime·mstart // // The new G calls runtime·main. func schedinit() { lockInit(\u0026sched.lock, lockRankSched) lockInit(\u0026sched.deferlock, lockRankDefer) lockInit(\u0026sched.sudoglock, lockRankSudog) lockInit(\u0026deadlock, lockRankDeadlock) lockInit(\u0026paniclk, lockRankPanic) lockInit(\u0026allglock, lockRankAllg) lockInit(\u0026allpLock, lockRankAllp) lockInit(\u0026reflectOffs.lock, lockRankReflectOffs) traceLockInit() ...... gp := getg() // 设置最大线程数为 10000，即 M 的最大数目 sched.maxmcount = 10000 crashFD.Store(^uintptr(0)) // The world starts stopped. worldStopped() ticks.init() // run as early as possible,用于性能分析和调度决策，需尽早运行 stackinit() // 栈初始化 randinit() // must run before mallocinit, alginit, mcommoninit，随机数初始化 mallocinit() // 内存分配器初始化 cpuinit(godebug) // must run before alginit alginit() // maps, hash, rand must not be used before this call，算法初始化（map、hash等） mcommoninit(gp.m, -1) // 初始化当前的操作系统线程（M0，即主线程）。 stkobjinit() // must run before GC starts，栈对象初始化 goargs() // 处理命令行参数 goenvs() // 处理环境变量 gcinit() // GC初始化 // Allocate stack space that can be used when crashing due to bad stack // conditions, e.g. morestack on g0. // 预分配 16KB 栈空间用于崩溃处理，如：当栈溢出或其他严重错误时，使用这个栈来输出错误信息 // stackguard 栈保护边界，用于检测栈溢出 gcrash.stack = stackalloc(16384) gcrash.stackguard0 = gcrash.stack.lo + 1000 gcrash.stackguard1 = gcrash.stack.lo + 1000 lock(\u0026sched.lock) // GOMAXPROCS 变量设置，如果未设置，使用 CPU 核心数 var procs int32 if n, ok := strconv.Atoi32(gogetenv(\"GOMAXPROCS\")); ok \u0026\u0026 n \u003e 0 { procs = n sched.customGOMAXPROCS = true } else { // Use numCPUStartup for initial GOMAXPROCS for two reasons: // // 1. We just computed it in osinit, recomputing is (minorly) wasteful. // // 2. More importantly, if debug.containermaxprocs == 0 \u0026\u0026 // debug.updatemaxprocs == 0, we want to guarantee that // runtime.GOMAXPROCS(0) always equals runtime.NumCPU (which is // just numCPUStartup). procs = defaultGOMAXPROCS(numCPUStartup) } // 调用 procresize 调整 P（处理器） if procresize(procs) != nil { throw(\"unknown runnable goroutine during bootstrap\") } unlock(\u0026sched.lock) // World is effectively started now, as P's can run. 标记运行时已启动，P 可以开始运行 worldStarted() ...... } P 的数量取决于当前 cpu 的数量，或者是 runtime.GOMAXPROCS 的配置。 几个重点： mallocinit()：内存分配器，初始化 mcache（每个 P 的本地缓存），分三级mcache、mcentral（中心缓存）、mheap（全局堆），在此之前不能进行堆内存分配 mcommoninit(gp.m, -1)：完善初始化当前 M（M0，主线程），将 M 加入全局 M 列表。前面 M0 已经存在：作为全局变量静态分配且在 rt0_go 汇编中与 g0 建立了关联，g0 和 m0 不是通过 malloc 动态分配的，而是编译时就分配好的静态内存，解决了“鸡生蛋”问题（在内存分配器初始化之前，我们需要一个 M 和 G 来运行初始化代码） gcinit()：初始化垃圾回收器的数据结构、工作线程、写屏障等 GOMAXPROCS 初始化 procresize(procs): 调整 P 的数目和 P 处理器的初始化。 重点看下procresize(procs): var allp []*p // Change number of processors. // // sched.lock must be held, and the world must be stopped. // // gcworkbufs must not be being modified by either the GC or the write barrier // code, so the GC must not be running if the number of Ps actually changes. // // Returns list of Ps with local work, they need to be scheduled by the caller. func procresize(nprocs int32) *p { assertLockHeld(\u0026sched.lock) assertWorldStopped() ...... old := gomaxprocs // Grow allp if necessary. if nprocs \u003e int32(len(allp)) { lock(\u0026allpLock) if nprocs \u003c= int32(cap(allp)) { allp = allp[:nprocs] } else { nallp := make([]*p, nprocs) copy(nallp, allp[:cap(allp)]) allp = nallp } unlock(\u0026allpLock) } // initialize new P's for i := old; i \u003c nprocs; i++ { pp := allp[i] if pp == nil { pp = new(p) } pp.init(i) // 设置id，初始化mcahe，设置状态为_Pgcstop atomicstorep(unsafe.Pointer(\u0026allp[i]), unsafe.Pointer(pp)) } gp := getg() if gp.m.p != 0 \u0026\u0026 gp.m.p.ptr().id \u003c nprocs { // continue to use the current P gp.m.p.ptr().status = _Prunning gp.m.p.ptr().mcache.prepareForSweep() } else { // release the current P and acquire allp[0]. // // We must do this before ","date":"2026-02-03","objectID":"/gmp/:2:2","tags":["Go"],"title":"GMP调度模型","uri":"/gmp/"},{"categories":["Go语言"],"content":"2.3 创建 main goroutine // 创建一个新的 goroutine 来运行 runtime.main，runtime.main 最终会调用用户的 main.main() // create a new goroutine to start program MOVQ $runtime·mainPC(SB), AX // runtime·mainPC 是一个全局变量，存储了 runtime.main 函数的地址 PUSHQ AX // 将 mainPC 压栈作为参数 CALL runtime·newproc(SB) POPQ AX // mainPC is a function value for runtime.main, to be passed to newproc. // The reference to runtime.main is made via ABIInternal, since the // actual function (not the ABI0 wrapper) is needed by newproc. DATA runtime·mainPC+0(SB)/8,$runtime·main\u003cABIInternal\u003e(SB) GLOBL runtime·mainPC(SB),RODATA,$8 等价于： // 只读全局变量，存储 runtime.main 函数的地址 var mainPC = (*funcval)(unsafe.Pointer(\u0026runtime.main)) 调用 newproc()，当写 go func() 时，编译器会将其转换为对 newproc 的调用。 // Create a new g running fn. // Put it on the queue of g's waiting to run. // The compiler turns a go statement into a call to this. func newproc(fn *funcval) { gp := getg() pc := sys.GetCallerPC() systemstack(func() { newg := newproc1(fn, gp, pc, false, waitReasonZero) pp := getg().m.p.ptr() runqput(pp, newg, true) if mainStarted { wakep() } }) } 2.3.1 systemstack切换g0栈 systemstack()，切换系统栈，保存当前 goroutine 的上下文，如果已经在 g0 或 gsignal 上，则直接执行，否则切换到 g0 栈，执行传入的函数，执行完毕后恢复原来的 goroutine 上下文。 // systemstack runs fn on a system stack. // If systemstack is called from the per-OS-thread (g0) stack, or // if systemstack is called from the signal handling (gsignal) stack, // systemstack calls fn directly and returns. // Otherwise, systemstack is being called from the limited stack // of an ordinary goroutine. In this case, systemstack switches // to the per-OS-thread stack, calls fn, and switches back. // It is common to use a func literal as the argument, in order // to share inputs and outputs with the code around the call // to system stack: // // ... set up y ... // systemstack(func() { // x = bigcall(y) // }) // ... use x ... // //go:noescape func systemstack(fn func()) 系统栈（g0 栈）上执行，空间更大，不会栈增长，避免在用户 goroutine 栈上执行可能因为栈增长的操作，导致的递归问题；不会触发 GC 扫描（g0 的栈不需要扫描），安全地操作 schd 调度器数据结构，不会被抢占，为 runtime 提供了一个稳定、安全、高效的执行环境。 2.3.2 新建一个 goroutine // Create a new g in state _Grunnable (or _Gwaiting if parked is true), starting at fn. // callerpc is the address of the go statement that created this. The caller is responsible // for adding the new g to the scheduler. If parked is true, waitreason must be non-zero. func newproc1(fn *funcval, callergp *g, callerpc uintptr, parked bool, waitreason waitReason) *g { // 防止创建一个没有执行函数的 goroutine，var f func() 和 go f() 等会触发 fatal error if fn == nil { fatal(\"go of nil func value\") } mp := acquirem() // disable preemption because we hold M and P in local vars. pp := mp.p.ptr() newg := gfget(pp) if newg == nil { newg = malg(stackMin) casgstatus(newg, _Gidle, _Gdead) allgadd(newg) // publishes with a g-\u003estatus of Gdead so GC scanner doesn't look at uninitialized stack. } if newg.stack.hi == 0 { throw(\"newproc1: newg missing stack\") } if readgstatus(newg) != _Gdead { throw(\"newproc1: new g is not Gdead\") } // g结构体的各个字段值的赋值 totalSize := uintptr(4*goarch.PtrSize + sys.MinFrameSize) totalSize = alignUp(totalSize, sys.StackAlign) sp := newg.stack.hi - totalSize memclrNoHeapPointers(unsafe.Pointer(\u0026newg.sched), unsafe.Sizeof(newg.sched)) newg.sched.sp = sp newg.stktopsp = sp newg.sched.pc = abi.FuncPCABI0(goexit) + sys.PCQuantum newg.sched.g = guintptr(unsafe.Pointer(newg)) gostartcallfn(\u0026newg.sched, fn) newg.parentGoid = callergp.goid newg.gopc = callerpc newg.ancestors = saveAncestors(callergp) newg.startpc = fn.fn var status uint32 = _Grunnable if parked { status = _Gwaiting newg.waitreason = waitreason } newg.goid = pp.goidcache casgstatus(newg, _Gdead, status) releasem(mp) return newg } 参数说明： fn *funcval：要执行的函数（包含函数指针和闭包变量） gp *g：父 goroutine（当前 goroutine） pc uintptr：调用者的 PC（创建位置），即 go 语句的位置 false：parked 参数，表示新 goroutine 不是处于等待状态 waitReasonZero：等待原因（这里为0，因为不等待） Part 1: 禁止当前 m 被抢占，绑定 m 和 p，从 p 的本地队列中获取空闲的 g，若没有则从全局队列中批量获取空闲的 g，仍然没有则创建一个 g，并分配栈空间，标记状态为 _Gdead，并添加到 allgs 中（全局变量，维持着所有的 g 数组，主要用来统计） mp := acquirem() // disabl","date":"2026-02-03","objectID":"/gmp/:2:3","tags":["Go"],"title":"GMP调度模型","uri":"/gmp/"},{"categories":["Go语言"],"content":"2.4 启动 GMP 调度循环 整体流程： 汇编： // start this M，启动调度 CALL runtime·mstart(SB) TEXT runtime·mstart(SB),NOSPLIT|TOPFRAME|NOFRAME,$0 CALL runtime·mstart0(SB) RET // not reached 在 runtime 中调用 mstart0，mstart0 中调用 mstart1，它的实现在 proc.go 中： func mstart1() { gp := getg() if gp != gp.m.g0 { throw(\"bad runtime·mstart\") } // 保存返回点，执行完后需要回到调用方的下一条语句上继续执行 gp.sched.g = guintptr(unsafe.Pointer(gp)) gp.sched.pc = sys.GetCallerPC() gp.sched.sp = sys.GetCallerSP() // 如果当前 m 不等于 m0，则需要绑定一个 p if gp.m != \u0026m0 { acquirep(gp.m.nextp.ptr()) gp.m.nextp = 0 } schedule() } schedule()：开启GMP调度，不在返回，循环执行下去 // One round of scheduler: find a runnable goroutine and execute it. // Never returns. func schedule() { mp := getg().m // 省略其他代码...... top: pp := mp.p.ptr() pp.preempt = false // Safety check: if we are spinning, the run queue should be empty. // Check this before calling checkTimers, as that might call // goready to put a ready goroutine on the local run queue. if mp.spinning \u0026\u0026 (pp.runnext != 0 || pp.runqhead != pp.runqtail) { throw(\"schedule: spinning with local work\") } // 找到一个可执行的goroutine gp, inheritTime, tryWakeP := findRunnable() // blocks until work is available // 解除M自旋 if mp.spinning { resetspinning() } // If about to schedule a not-normal goroutine (a GCworker or tracereader), // wake a P if there is one. if tryWakeP { wakep() } // 执行 goroutine execute(gp, inheritTime) } 2.4.1 findRunnable() 函数返回值，分别代表： gp：找到的 goroutine inheritTime：是否继承时间片 tryWakeP：是否需要唤醒其他 P findRunnable() 查找顺序： 清理和初始化 ├─ 清除 allp 快照 ├─ 检查 GC 等待 → gcstopm() → goto top ├─ 检查安全点函数 → runSafePointFn() └─ 检查定时器 特殊 goroutine（优先级最高） ├─ Trace reader (tryWakeP=true) └─ GC worker (tryWakeP=true) 全局队列公平性检查（每 61 次） └─ schedtick % 61 == 0 → globrunqget() 唤醒特殊 goroutine ├─ Finalizer goroutine └─ GC cleanup goroutine 本地队列（快速路径） └─ runqget(pp) → 优先检查 runnext 全局队列（批量获取） └─ globrunqgetbatch() → 获取一半到本地队列 网络轮询（非阻塞） └─ netpoll(0) → 检查就绪的网络 I/O 工作窃取 └─ stealWork() → 从其他 P 窃取 空闲 GC 标记 └─ 如果有 GC 工作，运行空闲标记 准备休眠 ├─ 释放 P ├─ 再次检查所有队列 ├─ 网络轮询（阻塞） └─ stopm() → 休眠 M func findRunnable() (gp *g, inheritTime, tryWakeP bool) { mp := getg().m top: // 获取 p pp := mp.p.ptr() if sched.gcwaiting.Load() { gcstopm() goto top } if gcBlackenEnabled != 0 { gp, tnow := gcController.findRunnableGCWorker(pp, now) if gp != nil { return gp, false, true } now = tnow } // 每 61 次调度，需要尝试处理一次全局队列 (防止饥饿) if pp.schedtick%61 == 0 \u0026\u0026 !sched.runq.empty() { lock(\u0026sched.lock) gp := globrunqget() unlock(\u0026sched.lock) if gp != nil { return gp, false, false } } // local runq if gp, inheritTime := runqget(pp); gp != nil { return gp, inheritTime, false } // global runq if !sched.runq.empty() { lock(\u0026sched.lock) gp, q := globrunqgetbatch(int32(len(pp.runq)) / 2) unlock(\u0026sched.lock) if gp != nil { if runqputbatch(pp, \u0026q); !q.empty() { throw(\"Couldn't put Gs into empty local runq\") } return gp, false, false } } // 从网络就绪io中读取，netpoll，返回io就绪的 g if netpollinited() \u0026\u0026 netpollAnyWaiters() \u0026\u0026 sched.lastpoll.Load() != 0 \u0026\u0026 sched.pollingNet.Swap(1) == 0 { list, delta := netpoll(0) sched.pollingNet.Store(0) if !list.empty() { // non-blocking gp := list.pop() injectglist(\u0026list) netpollAdjustWaiters(delta) casgstatus(gp, _Gwaiting, _Grunnable) return gp, false, false } } // Spinning Ms: steal work from other Ps. // // Limit the number of spinning Ms to half the number of busy Ps. // This is necessary to prevent excessive CPU consumption when // GOMAXPROCS\u003e\u003e1 but the program parallelism is low. if mp.spinning || 2*sched.nmspinning.Load() \u003c gomaxprocs-sched.npidle.Load() { if !mp.spinning { mp.becomeSpinning() } // 窃取其他 p 的 g gp, inheritTime, tnow, w, newWork := stealWork(now) if gp != nil { // Successfully stole. return gp, inheritTime, false } } ....... // We have nothing to do. // 若存在 gc 并发标记任务，做GC标记工作，好过直接回收 p // If we're in the GC mark phase, can safely scan and blacken objects, // and have work to do, run idle-time marking rather than give up the P. if gcBlackenEnabled != 0 \u0026\u0026 gcMarkWorkAvailable(pp) \u0026\u0026 gcController.addIdleMarkWorker() { node := (*gcBgMarkWork","date":"2026-02-03","objectID":"/gmp/:2:4","tags":["Go"],"title":"GMP调度模型","uri":"/gmp/"},{"categories":["Go语言"],"content":"3、让渡设计 ","date":"2026-02-03","objectID":"/gmp/:3:0","tags":["Go"],"title":"GMP调度模型","uri":"/gmp/"},{"categories":["Go语言"],"content":"3.1 g执行完后让渡 g 执行完后退出 自动调用 runtime·goexit，进而 runtime·goexit1，在 runtime·goexit1 中通过 mcall(goexit0) 切换到 g0，调用goexit0 TEXT runtime·goexit(SB),NOSPLIT|TOPFRAME|NOFRAME,$0-0 BYTE $0x90 // NOP CALL runtime·goexit1(SB) // does not return // goexit continuation on g0. func goexit0(gp *g) { gdestroy(gp) schedule() } func gdestroy(gp *g) { mp := getg().m pp := mp.p.ptr() // 状态切换 casgstatus(gp, _Grunning, _Gdead) // 解除 M ↔ G 的关联 gp.m = nil locked := gp.lockedm != 0 gp.lockedm = 0 mp.lockedg = 0 // 清理 goroutine 的各种字段 gp.preemptStop = false gp.paniconfault = false gp._defer = nil // should be true already but just in case. gp._panic = nil // non-nil for Goexit during panic. points at stack-allocated data. gp.writebuf = nil gp.waitreason = waitReasonZero gp.param = nil gp.labels = nil gp.timer = nil // 解除当前 M 和 G 的关联，清除双向指针：M.curg 和 G.m dropg() // 加入到 p 的本地空闲 gFree队列，当创建新的 goroutine 时（newproc1），会先从空闲列表获取 gfput(pp, gp) } ","date":"2026-02-03","objectID":"/gmp/:3:1","tags":["Go"],"title":"GMP调度模型","uri":"/gmp/"},{"categories":["Go语言"],"content":"3.2 g主动让渡 g主动让渡指的是由用户手动调用 runtime.Gosched 方法让出 g 所持有的执行权。在 Gosched 方法中，会通过 mcall 指令切换至 g0，并由 g0 执行 gosched_m 方法，其中包含如下步骤： 将 g 由 running 改为 runnable 状态 解除 g 和 m 的关系 将 g 直接添加到全局队列 grq 中 调用 schedule 方法发起新一轮调度 // 主动让渡出执行权，此时执行方还是普通 g func Gosched() { // 通过 mcall，将执行方转为 g0，调用 gosched_m 方法 mcall(gosched_m) } // 将 gp 切换回就绪态后添加到全局队列 grq，并发起新一轮调度 // 此时执行方为 g0 func gosched_m(gp *g) { // ... goschedImpl(gp) } func goschedImpl(gp *g) { // 将 g 状态由 running 改为 runnable 就绪态 casgstatus(gp, _Grunning, _Grunnable) // 解除 g 和 m 的关系 dropg() // 将 g 添加到全局队列 grq lock(\u0026sched.lock) globrunqput(gp) unlock(\u0026sched.lock) // 发起新一轮调度 schedule() } ","date":"2026-02-03","objectID":"/gmp/:3:2","tags":["Go"],"title":"GMP调度模型","uri":"/gmp/"},{"categories":["Go语言"],"content":"3.3 g阻塞让渡 G阻塞让渡指的是 g 在执行过程中有需要等待的外部资源，需要进入阻塞等待的状态（waiting），直到条件达成后才能完成将状态重新更新为就绪态（runnable），如：Channel 操作阻塞、Select 语句阻塞、网络 I/O 等待 通过 runtime/proc.go 的 gopark 方法实现： 通过 mcall 从 g 切换至 g0，并由 g0 执行 park_m 方法 g0 将 g 由 running 更新为 waiting 状态，然后发起新一轮调度 // 此时执行方为普通 g func gopark(unlockf func(*g, unsafe.Pointer) bool, lock unsafe.Pointer, reason waitReason, traceEv byte, traceskip int) { // 获取 m 正在执行的 g，也就是要阻塞让渡的 g mp := acquirem() gp := mp.curg ....... // 通过 mcall，将执行方由普通 g -\u003e g0 mcall(park_m) } // 此时执行方为 g0. 入参 gp 为需要执行 park 的普通 g func park_m(gp *g) { // 获取 m mp := getg().m // 将 gp 状态由 running 变更为 waiting casgstatus(gp, _Grunning, _Gwaiting) // 解绑 g 与 m 的关系 dropg() // g0 发起新一轮调度流程 schedule() } 相反，唤醒 g 的方法是 goready 方法，通过 systemstack 压栈切换至 g0 执行 ready 方法——将目标 g 状态由 waiting 改为 runnable，然后添加到就绪队列中。 // 此时执行方为普通 g. 入参 gp 为需要唤醒的另一个普通 g func goready(gp *g, traceskip int) { // 调用 systemstack 后，会切换至 g0 调用传入的 ready 方法. 调用结束后则会直接切换回到当前普通 g 继续执行. systemstack(func() { ready(gp, traceskip, true) }) // 恢复成普通 g 继续执行 ... } // 此时执行方为 g0. 入参 gp 为拟唤醒的普通 g func ready(gp *g, traceskip int, next bool) { mp := acquirem() casgstatus(gp, _Gwaiting, _Grunnable) /* 1) 优先将目标 g 添加到当前 p 的本地队列 lrq 2）若 lrq 满了，则将 g 追加到全局队列 grq */ runqput(mp.p.ptr(), gp, next) // 如果有 m 或 p 处于 idle 状态，将其唤醒 wakep() } ","date":"2026-02-03","objectID":"/gmp/:3:3","tags":["Go"],"title":"GMP调度模型","uri":"/gmp/"},{"categories":["Go语言"],"content":"4、抢占设计 抢占和让渡有相同之处，都表示由 g-\u003eg0 的流转过程，但区别在于，让渡是由 g 主动发起的（第一人称），而抢占则是由外力干预（sysmon thread监控线程）发起的（第三人称）。 程序启动创建 main goroutine时，在运行时的 main 函数中，会启动一个全局唯一的监控线程 sysmon thread，定时执行监控工作，包括： 从网络轮询中将就绪的 g 放到运行队列 retake 抢占长时间运行的 Goroutine 和夺回阻塞的 P func main() { mainStarted = true if haveSysmon { systemstack(func() { newm(sysmon, nil, -1) // 通过newm创建新线程，执行sysmon函数，ID 为 -1，标记为系统线程 }) } } // Always runs without a P, so write barriers are not allowed. //go:nowritebarrierrec func sysmon() { lock(\u0026sched.lock) sched.nmsys++ // 增加系统 M 计数 unlock(\u0026sched.lock) ...... for { // 根据闲忙情况调整轮询间隔，在空闲情况下 10 ms 轮询一次 if idle == 0 { delay = 20 // 开始时睡眠 20 微秒 } else if idle \u003e 50 { delay *= 2 // 1ms 后开始加倍睡眠时间 } if delay \u003e 10*1000 { delay = 10 * 1000 // 最多睡眠 10ms } usleep(delay) // 将就绪的网络 goroutine 重新放入运行队列 // poll network if not polled for more than 10ms lastpoll := sched.lastpoll.Load() if netpollinited() \u0026\u0026 lastpoll != 0 \u0026\u0026 lastpoll+10*1000*1000 \u003c now { sched.lastpoll.CompareAndSwap(lastpoll, now) list, delta := netpoll(0) // non-blocking - returns list of goroutines if !list.empty() { incidlelocked(-1) injectglist(\u0026list) incidlelocked(1) netpollAdjustWaiters(delta) } } // 抢占长时间运行的 Goroutine 和夺回阻塞的 P // retake P's blocked in syscalls // and preempt long running G's if retake(now) != 0 { idle = 0 // 有工作被抢占，重置空闲计数 } else { idle++ } // check if we need to force a GC if t := (gcTrigger{kind: gcTriggerTime, now: now}); t.test() \u0026\u0026 forcegc.idle.Load() { .... } unlock(\u0026sched.sysmonlock) } } 下面主要关注 retake： func retake(now int64) uint32 { n := 0 for i := 0; i \u003c len(allp); i++ { pp := allp[i] pd := \u0026pp.sysmontick s := pp.status // 情况 1: P 正在运行或在系统调用中 if s == _Prunning || s == _Psyscall { t := int64(pp.schedtick) if int64(pd.schedtick) != t { // schedtick 变化了，说明有新的 G 在运行 pd.schedtick = uint32(t) pd.schedwhen = now } else if pd.schedwhen+forcePreemptNS \u003c= now { // 同一个 G 运行超过 10ms，抢占它 preemptone(pp) sysretake = true } } // 情况 2: P 在系统调用中 if s == _Psyscall { t := int64(pp.syscalltick) if !sysretake \u0026\u0026 int64(pd.syscalltick) != t { pd.syscalltick = uint32(t) pd.syscallwhen = now continue } // 系统调用超过 20us，且有其他工作要做 if runqempty(pp) \u0026\u0026 sched.nmspinning.Load()+sched.npidle.Load() \u003e 0 \u0026\u0026 pd.syscallwhen+10*1000*1000 \u003e now { continue // P 的队列为空，不夺回 } // 夺回 P if atomic.Cas(\u0026pp.status, s, _Pidle) { n++ pp.syscalltick++ handoffp(pp) // 将 P 交给其他 M } } } return uint32(n) } 两种抢占场景： 场景 A：抢占长时间运行的 Goroutine 场景 B：夺回阻塞在系统调用中的 P ","date":"2026-02-03","objectID":"/gmp/:4:0","tags":["Go"],"title":"GMP调度模型","uri":"/gmp/"},{"categories":["Go语言"],"content":"4.1 超时抢占 时间阈值：10 毫秒（forcePreemptNS = 10 * 1000 * 1000） 触发条件：同一个 G（或通过 runnext 共享时间片的 G 序列）运行超过 10ms 抢占方式： func preemptone(pp *p) bool { mp := pp.m.ptr() // 获取 p 上正在执行的 g（抢占目标） gp := mp.curg gp.preempt = true gp.stackguard0 = stackPreempt // 设置栈边界为stackPreempt，当g执行时触发栈检查。 // 异步抢占（发送信号）会对目标 g 所在的 m 发送抢占信号 sigPreempt，通过改写 g 程序计数器（pc，program counter）的方式将 g 逼停 if preemptMSupported \u0026\u0026 debug.asyncpreemptoff == 0 { pp.preempt = true preemptM(mp) // 发送 SIGURG 信号 } return true } // 发送抢占信号 func preemptM(mp *m) { ....... if mp.signalPending.CompareAndSwap(0, 1) { signalM(mp, sigPreempt) } } func signalM(mp *m, sig int) { pthread_kill(pthread(mp.procid), uint32(sig)) } ","date":"2026-02-03","objectID":"/gmp/:4:1","tags":["Go"],"title":"GMP调度模型","uri":"/gmp/"},{"categories":["Go语言"],"content":"4.2 系统调用抢占 在发起系统调用时，会执行位于 runtime/proc.go 的 reentersyscall 方法，此方法核心步骤包括： 将 g 和 p 的状态更新为 syscall 解除 p 和 m 的绑定 将 p 设置为 m.oldp，保留 p 与 m 之间的弱联系（使得 m syscall 结束后，还有一次尝试复用 p 的机会） func reentersyscall(pc, sp uintptr) { // 获取 g gp := getg() // 保存寄存器信息 save(pc, sp) // ... // 将 g 状态更新为 syscall casgstatus(_g_, _Grunning, _Gsyscall) // ... // 解除 p 与 m 绑定关系 pp := gp.m.p.ptr() pp.m = 0 // 将 p 设置为 m 的 oldp gp.m.oldp.set(pp) gp.m.p = 0 // 将 p 状态更新为 syscall atomic.Store(\u0026pp.status, _Psyscall) } 当系统系统调用完成时，会执行位于 runtime/proc.go 的 exitsyscall 方法（此时执行方还是 m 上的 g），包含如下步骤： 检查 syscall 期间，p 是否未和其他 m 结合，如果是的话，直接复用 p，继续执行 g 通过 mcall 操作切换至 g0 执行 exitsyscall0 方法，尝试为当前 m 结合一个新的 p，如果结合成功，则继续执行 g，否则将 g 添加到 grq 后暂停 m func exitsyscall() { // 获取 g gp := getg() // ... // 如果 oldp 没有和其他 m 结合，则直接复用 oldp oldp := _g_.m.oldp.ptr() _g_.m.oldp = 0 if exitsyscallfast(oldp) { // ... // 将 g 状态由 syscall 更新回 running casgstatus(_g_, _Gsyscall, _Grunning) // ... return } // 切换至 g0 调用 exitsyscall0 方法 mcall(exitsyscall0) } // 此时执行方为 m 下的 g0 func exitsyscall0(gp *g) { // 将 g 的状态修改为 runnable 就绪态 casgstatus(gp, _Gsyscall, _Grunnable) // 解除 g 和 m 的绑定关系 dropg() lock(\u0026sched.lock) // 尝试寻找一个空闲的 p 与当前 m 结合 var pp *p if schedEnabled(gp) { pp, _ = pidleget(0) } var locked bool // 如果与 p 结合失败，则将 g 添加到全局队列中 if pp == nil { globrunqput(gp) // ... } // ... unlock(\u0026sched.lock) // 如果与 p 结合成功，则继续调度 g if pp != nil { acquirep(pp) execute(gp, false) // Never returns. } // ... // 与 p 结合失败的话，需要将当前 m 添加到 schedt 的 midle 队列并停止 m stopm() // 如果 m 被重新启用，则发起新一轮调度 schedule() // Never returns. } 回到 sysmon thread 中的 retake 方法，此处会遍历每个 p，并针对正在发起系统调用的 p 执行如下检查逻辑： 检查 p 的 lrq 中是否存在等待执行的 g 检查 p 的 syscall 时长是否 \u003e= 10ms 处理方式： 将 P 的状态改为 _Pidle 调用 handoffp(pp) 将 P 交给其他 M 使用 原来的 M 从系统调用返回后会寻找新的 P func retake(now int64) uint32 { // 情况 2: P 在系统调用中 if s == _Psyscall { t := int64(pp.syscalltick) if !sysretake \u0026\u0026 int64(pd.syscalltick) != t { pd.syscalltick = uint32(t) pd.syscallwhen = now continue } // 系统调用超过 20us，且有其他工作要做 if runqempty(pp) \u0026\u0026 sched.nmspinning.Load()+sched.npidle.Load() \u003e 0 \u0026\u0026 pd.syscallwhen+10*1000*1000 \u003e now { continue // P 的队列为空，不夺回 } // 夺回 P if atomic.Cas(\u0026pp.status, s, _Pidle) { n++ pp.syscalltick++ handoffp(pp) // 将 P 交给其他 M } } } func handoffp(_p_ *p) { // 如果 p lrq 中还有 g 或者全局队列 grq 中还有 g，则立即分配一个新 m 与该 p 结合 if !runqempty(_p_) || sched.runqsize != 0 { // 分配一个 m 与 p 结合 startm(_p_, false) return } // ... // 若系统空闲没有 g 需要调度，则将 p 添加到 schedt 中的空闲 p 队列 pidle 中 pidleput(_p_, 0) // ... } ","date":"2026-02-03","objectID":"/gmp/:4:2","tags":["Go"],"title":"GMP调度模型","uri":"/gmp/"},{"categories":["Go语言"],"content":"其他 ","date":"2026-02-03","objectID":"/gmp/:5:0","tags":["Go"],"title":"GMP调度模型","uri":"/gmp/"},{"categories":["Go语言"],"content":"1 补充 1.1 g 栈空间分布： 栈大小计算： stack.hi - stack.lo = 2048 字节 (初始栈大小，即 stackMin) 如果 SP \u003c stackguard0: 触发栈增长（调用 runtime.morestack） 分配更大的栈 复制旧栈内容到新栈 继续执行 除了栈溢出检测，stackguard0 还用于实现抢占。 gp.stackguard0 = stackPreempt // 0xfffffade（一个很大的值） 下次函数调用时： SP (正常值) \u003c stackPreempt (超大值) → 触发 morestack morestack 检测到 stackguard0 == stackPreempt 执行抢占逻辑而非栈增长 1.2 goroutine 状态转换 1.3 抢占禁用p的next槽位 // Goroutine A for { ch \u003c- 1 // 发送后立即阻塞，没有函数调用 } // Goroutine B for { \u003c-ch // 接收后立即阻塞，没有函数调用 } 上述例子中会出现 A B 互相唤醒，互相执行，继承 p 的时间片一直执行下去，导致 p 上的其他 g 处于饥饿状态，时间片一直会被刷新，是因为从 runqget 中获取 g 时，如果 next 有值，则优先返回，并且 inheritTime = true,标志继承时间片。 func runqget(pp *p) (gp *g, inheritTime bool) { // 如果 runnext 有值，优先返回它 if next != 0 \u0026\u0026 pp.runnext.cas(next, 0) { return next.ptr(), true // ← inheritTime = true } // 否则从普通队列取 // ... return gp, false // ← inheritTime = false } 在调度器执行时，会判断 inheritTime 标识是否为 true，为 true 则调度器计数器不增加。 func execute(gp *g, inheritTime bool) { mp := getg().m // ... 省略其他代码 ... if !inheritTime { mp.p.ptr().schedtick++ // ← 只有 inheritTime=false 时才增加 schedtick } // ... 省略其他代码 ... gogo(\u0026gp.sched) // 开始执行 goroutine } 因此，在 runqput 中如果没有系统监控主动抢占时，需要禁止 next。 系统监控抢占是通过 sysmon 检查 pp.schedtick 是否变化来判断，如果 schedtick 长时间不变，说明时间片用完，触发抢占。 // sysmon 每 10ms 检查一次 func retake(now int64) uint32 { // ... if s == _Prunning || s == _Psyscall { // Preempt G if it's running on the same schedtick for // too long. This could be from a single long-running // goroutine or a sequence of goroutines run via // runnext, which share a single schedtick time slice. t := int64(pp.schedtick) if int64(pd.schedtick) != t { pd.schedtick = uint32(t) pd.schedwhen = now } else if pd.schedwhen+forcePreemptNS \u003c= now { preemptone(pp) // ← 抢占 } } // ... } 1.4 完整启动流程 时间线： T0: 程序启动 ↓ T1: 汇编入口（asm_amd64.s） ├─ mainStarted = false（默认值） ├─ newproc(runtime.main) \u003c-创建 main goroutine │ ├─ runqput(P0, main_g, true) \u003c-放入 runnext │ └─ if mainStarted { wakep() } \u003c-不执行！ └─ mstart() \u003c-启动 M0 T2: M0 进入调度循环 ├─ mstart0() ├─ mstart1() └─ schedule() \u003c-永不返回 T3: schedule() 查找 goroutine ├─ findRunnable() ├─ runqget(P0) \u003c-从 runnext 取出 main goroutine └─ execute(main_g, inheritTime=true) \u003c-继承时间片 T4: 执行 runtime.main ├─ mainStarted = true ├─ 启动 sysmon ├─ 执行 init 函数 └─ 执行 main.main() ← 执行用户代码 T5: 用户代码中创建 goroutine ├─ go func() { … } ├─ newproc(fn) ├─ runqput(P0, new_g, true) └─ if mainStarted { wakep() } \u003c-唤醒 m 执行 g 1.5 一个阻塞 channel 的生命周期 Goroutine 尝试从空 channel 接收 发现没有数据，需要阻塞 调用 gopark() → mcall(park_m) park_m(): casgstatus(gp, _Grunning, _Gwaiting) // 状态转换 dropg() // 解除 M ↔ G 关联 schedule() // 调度下一个 goroutine M 继续执行其他 goroutine 另一个 goroutine 向 channel 发送数据 调用 goready(gp) 唤醒阻塞的 goroutine gp 被放入运行队列 某个 M 通过 schedule() → findrunnable() 获取 gp execute(gp): mp.curg = gp // 重新建立 M → G 关联 gp.m = mp // 重新建立 G → M 关联 gogo(\u0026gp.sched) // 恢复执行 ","date":"2026-02-03","objectID":"/gmp/:5:1","tags":["Go"],"title":"GMP调度模型","uri":"/gmp/"},{"categories":["Go语言"],"content":"2 GMP 模型的进化思想 在高并发场景下，大量的线程创建、使用、切换、销毁会占用大量的内存，并浪费 CPU 时间工作在非工作任务的上，导致程序并发处理请求的能力降低。 ","date":"2026-02-03","objectID":"/gmp/:5:2","tags":["Go"],"title":"GMP调度模型","uri":"/gmp/"},{"categories":["Go语言"],"content":"3 GMP 生态 go 语言中的并发工具都是以 G 为并发粒度打造的，围绕GMP构造的并发世界 锁 mutex、通道 channel 等都是基于 GMP 适配，在执行阻塞操作时，是阻塞在 g 的粒度，而非 m 粒度，因此这里的唤醒与阻塞都在用户态完成，无需内核介入，同时阻塞一个 g 也不会影响 m 下其他 g 的运行。 在网络 io 模型方面，采用 epoll 多路复用技术，epoll_wait 阻塞在 m（thread）粒度，golang 专门设计一套 netpoll 机制，使用用户态的 gopark 指令实现阻塞操作，使用非阻塞 epoll_wait 结合用户态的 goready 指令实现唤醒操作，从而将 io 行为也控制在 g 粒度。 内存管理，继承 TCMalloc（Thread-Caching-Malloc）思路，为每一个 p 都准备了一份私有的高速缓冲 mcahe，这部分内存分配可以使得 p 无锁化。 ","date":"2026-02-03","objectID":"/gmp/:5:3","tags":["Go"],"title":"GMP调度模型","uri":"/gmp/"},{"categories":["数据库"],"content":"go 标准库 mysql 驱动底层实现 一、mysql 驱动基本架子 ","date":"2026-02-03","objectID":"/go_sql_drive_mysql/:0:0","tags":["GORM","Go"],"title":"mysql驱动","uri":"/go_sql_drive_mysql/"},{"categories":["数据库"],"content":"1、驱动打开连接 mysql 驱动的实现库：github.com/go-sql-driver/mysql 基本用法： import ( _ \"github.com/go-sql-driver/mysql\" ) 在库的 init 函数中会注册驱动到前一节提到的 go 官方数据库 var driverName = \"mysql\" func init() { if driverName != \"\" { sql.Register(driverName, \u0026MySQLDriver{}) } } mysql 库实现了 Driver 接口的驱动结构体是 MySQLDriver{} type Driver interface { // 开启一个新的数据库连接 Open(name string) (Conn, error) } // Open new Connection. // See https://github.com/go-sql-driver/mysql#dsn-data-source-name for how // the DSN string is formatted func (d MySQLDriver) Open(dsn string) (driver.Conn, error) { cfg, err := ParseDSN(dsn) if err != nil { return nil, err } c := newConnector(cfg) return c.Connect(context.Background()) } 分为两步： 1、解析 dsn 字符串为配置对象 2、使用 cfg 配置对象新建一个连接器对象，并调用连接器对象的 Connect 方法创建一个连接 ","date":"2026-02-03","objectID":"/go_sql_drive_mysql/:1:0","tags":["GORM","Go"],"title":"mysql驱动","uri":"/go_sql_drive_mysql/"},{"categories":["数据库"],"content":"1.解析配置 dsn 字符串的总体形式： [user[:password]@][net[(addr)]]/dbname[?param1=value1\u0026paramN=valueN] 解析 dsn 字符串主要通过按照不同的特殊符号（@，/，？）等分割字符串，取出对应的值保存到 Config 对象中,并对一些未给值的赋予默认值 Config 结构体： type Config struct { // non boolean fields User string // Username Passwd string // Password (requires User) Net string // Network (e.g. \"tcp\", \"tcp6\", \"unix\". default: \"tcp\") Addr string // Address (default: \"127.0.0.1:3306\" for \"tcp\" and \"/tmp/mysql.sock\" for \"unix\") DBName string // Database name Params map[string]string // Connection parameters ConnectionAttributes string // Connection Attributes, comma-delimited string of user-defined \"key:value\" pairs Collation string // Connection collation Loc *time.Location // Location for time.Time values MaxAllowedPacket int // Max packet size allowed ServerPubKey string // Server public key name TLSConfig string // TLS configuration name TLS *tls.Config // TLS configuration, its priority is higher than TLSConfig Timeout time.Duration // Dial timeout ReadTimeout time.Duration // I/O read timeout WriteTimeout time.Duration // I/O write timeout Logger Logger // Logger // boolean fields AllowAllFiles bool // Allow all files to be used with LOAD DATA LOCAL INFILE AllowCleartextPasswords bool // Allows the cleartext client side plugin AllowFallbackToPlaintext bool // Allows fallback to unencrypted connection if server does not support TLS AllowNativePasswords bool // Allows the native password authentication method AllowOldPasswords bool // Allows the old insecure password method CheckConnLiveness bool // Check connections for liveness before using them ClientFoundRows bool // Return number of matching rows instead of rows changed ColumnsWithAlias bool // Prepend table alias to column names InterpolateParams bool // Interpolate placeholders into query string MultiStatements bool // Allow multiple statements in one query ParseTime bool // Parse time values to time.Time RejectReadOnly bool // Reject read-only connections // unexported fields. new options should be come here beforeConnect func(context.Context, *Config) error // Invoked before a connection is established pubKey *rsa.PublicKey // Server public key timeTruncate time.Duration // Truncate time.Time values to the specified duration } ","date":"2026-02-03","objectID":"/go_sql_drive_mysql/:1:1","tags":["GORM","Go"],"title":"mysql驱动","uri":"/go_sql_drive_mysql/"},{"categories":["数据库"],"content":"2.新建连接 func newConnector(cfg *Config) *connector { encodedAttributes := encodeConnectionAttributes(cfg) return \u0026connector{ cfg: cfg, encodedAttributes: encodedAttributes, } } func encodeConnectionAttributes(cfg *Config) string { connAttrsBuf := make([]byte, 0) // default connection attributes connAttrsBuf = appendLengthEncodedString(connAttrsBuf, connAttrClientName) connAttrsBuf = appendLengthEncodedString(connAttrsBuf, connAttrClientNameValue) connAttrsBuf = appendLengthEncodedString(connAttrsBuf, connAttrOS) connAttrsBuf = appendLengthEncodedString(connAttrsBuf, connAttrOSValue) connAttrsBuf = appendLengthEncodedString(connAttrsBuf, connAttrPlatform) connAttrsBuf = appendLengthEncodedString(connAttrsBuf, connAttrPlatformValue) connAttrsBuf = appendLengthEncodedString(connAttrsBuf, connAttrPid) connAttrsBuf = appendLengthEncodedString(connAttrsBuf, strconv.Itoa(os.Getpid())) serverHost, _, _ := net.SplitHostPort(cfg.Addr) if serverHost != \"\" { connAttrsBuf = appendLengthEncodedString(connAttrsBuf, connAttrServerHost) connAttrsBuf = appendLengthEncodedString(connAttrsBuf, serverHost) } // user-defined connection attributes for _, connAttr := range strings.Split(cfg.ConnectionAttributes, \",\") { k, v, found := strings.Cut(connAttr, \":\") if !found { continue } connAttrsBuf = appendLengthEncodedString(connAttrsBuf, k) connAttrsBuf = appendLengthEncodedString(connAttrsBuf, v) } return string(connAttrsBuf) } 这段代码的核心目的是在建立 MySQL 连接时，按照协议规范将客户端信息（默认属性）和用户自定义属性编码为特定格式的字节流。这些属性会在握手阶段发送给服务器，用于： 服务端监控：识别客户端类型、操作系统、PID 等。 审计跟踪：记录连接来源和自定义元数据。 协议兼容性：确保客户端与不同版本 MySQL 服务器的兼容性。 通过逐层编码，最终生成一个符合 MySQL 协议的连接属性字符串，存储在 connector 中供后续连接使用。 将字符串编码为 长度前缀 + 原始内容 格式，例如字符串 \"abc\" 编码为 0x03 'a' 'b' 'c' ","date":"2026-02-03","objectID":"/go_sql_drive_mysql/:1:2","tags":["GORM","Go"],"title":"mysql驱动","uri":"/go_sql_drive_mysql/"},{"categories":["数据库"],"content":"2.mysqlConn 结构体 type mysqlConn struct { buf buffer // 读写缓冲区 netConn net.Conn // 当前活跃的网络连接对象，可能是普通的TCP连接或者TLS加密连接 rawConn net.Conn // underlying connection when netConn is TLS connection.当 netConn 是 TLS 连接时，保存底层原始 TCP 连接（用于需要绕过 TLS 的场景，如连接超时设置）。比如设置 SO_TIMEOUT 等底层 Socket 参数时需操作 rawConn result mysqlResult // managed by clearResult() and handleOkPacket(). cfg *Config connector *connector // 连接器 maxAllowedPacket int // 服务器允许的最大数据包大小（单位：字节），用于限制客户端发送的单个数据包体积，从服务器握手阶段返回的 max_allowed_packet 值初始化 maxWriteSize int // 客户端单次写入网络的最大数据块大小，用于优化大块数据的分片发送 writeTimeout time.Duration // 写入操作的超时时间 flags clientFlag // 客户端能力标志位，记录与服务器协商后的支持特性（如是否启用 SSL、是否支持多语句等） status statusFlag // 连接状态标志位，表示服务器返回的当前状态（如事务是否活跃、是否有未读取的结果集） sequence uint8 // 数据包序列号（0~255 循环），用于保证客户端与服务器之间数据包的顺序一致性，递增 parseTime bool // for context support (Go 1.8+) watching bool // 标记是否正在通过 watcher 通道监听上下文取消事件 watcher chan\u003c- context.Context // 单向通道（只发送），用于向外部通知上下文取消事件 closech chan struct{} // 通知连接关闭的内部通道。关闭此通道表示连接已终止 finished chan\u003c- struct{} // 单向通道（只发送），用于通知外部调用者连接已完全关闭 canceled atomicError // set non-nil if conn is canceled 原子存储的取消错误，记录连接被取消的原因 closed atomicBool // set when conn is closed, before closech is closed 原子标记连接是否已关闭，避免重复关闭导致的 panic } mysqlConn 结构体封装了一个 MySQL 连接的完整状态，包括网络层、协议控制、数据结果、超时管理、上下文生命周期等。 调用 connector.Connect() 方法创建一个 mysql 连接，该连接使用结构体 mysqlConn 表示，它实现了 sql 标准库的 driver.Conn 接口 type Conn interface { // Prepare returns a prepared statement, bound to this connection. Prepare(query string) (Stmt, error) Close() error Begin() (Tx, error) } ","date":"2026-02-03","objectID":"/go_sql_drive_mysql/:2:0","tags":["GORM","Go"],"title":"mysql驱动","uri":"/go_sql_drive_mysql/"},{"categories":["数据库"],"content":"1.client与server端数据交互协议 在看跟 mysql 服务端交互逻辑之前，先熟悉 mysql 客户端跟服务端之前的数据交互协议： 每笔消息分为请求头和正文两部分 在请求头部分中： 前三个字节对应的是消息正文长度，共 24 个 bit 位，表示的长度最大值为 2^24 - 1，因此消息最大长度为 16MB-1byte. 如果消息长度大于该阈值，则需要进行分包 第四个字节对应为请求的 sequence 序列号. 一个新的客户端从 0 开始依次递增序列号，每次读消息时，会对序列号进行校验，要求必须必须和本地序号保持一致 在正文部分中： 对于客户端接收服务端消息的场景，首个字节标识了这条消息的状态. 倘若为 0，代表响应成功；倘若为 255，代表有错误发生；其他枚举值含义此处不再赘述 对于客户端发送消息到服务端的场景，首个字节标识了这笔请求的类型. 首个字节代表的是 sql 指令的类型。 ","date":"2026-02-03","objectID":"/go_sql_drive_mysql/:2:1","tags":["GORM","Go"],"title":"mysql驱动","uri":"/go_sql_drive_mysql/"},{"categories":["数据库"],"content":"2.读写缓冲区 读数据包： // Read packet to buffer 'data' func (mc *mysqlConn) readPacket() ([]byte, error) { var prevData []byte // 存数据包分片的缓冲区 for { // read packet header 包头 data, err := mc.buf.readNext(4) // 读取失败，关闭连接 if err != nil { if cerr := mc.canceled.Value(); cerr != nil { return nil, cerr } mc.log(err) mc.Close() return nil, ErrInvalidConn } // packet length [24 bit] // 读前三个字节表示包的长度 pktLen := int(uint32(data[0]) | uint32(data[1])\u003c\u003c8 | uint32(data[2])\u003c\u003c16) // check packet sync [8 bit] // 校验第四个字节的服务端包序列号是否跟本地的序列号一致 if data[3] != mc.sequence { mc.Close() // 出现跳包 // ErrPktSync = errors.New(\"commands out of sync. You can't run this command now\") // ErrPktSyncMul = errors.New(\"commands out of sync. Did you run multiple statements at once?\") if data[3] \u003e mc.sequence { return nil, ErrPktSyncMul } return nil, ErrPktSync } // 序号递增，一个字节8位最多能表示256个序号，从0到255 mc.sequence++ // packets with length 0 terminate a previous packet which is a // multiple of (2^24)-1 bytes long // 包长为0，说明前面的包都已经读取完成，MySQL 协议规定：0长度包表示分片结束 if pktLen == 0 { // there was no previous packet // 前面必须要读到数据，没有则报错，关闭连接 if prevData == nil { mc.log(ErrMalformPkt) mc.Close() return nil, ErrInvalidConn } // 返回成功读取的数据 return prevData, nil } // read packet body [pktLen bytes] // 读取 pktLen 长度的数据出来 data, err = mc.buf.readNext(pktLen) if err != nil { if cerr := mc.canceled.Value(); cerr != nil { return nil, cerr } mc.log(err) mc.Close() return nil, ErrInvalidConn } // return data if this was the last packet // 如果该包长度小于 2^24 - 1 = 16MB, 则是最后一个包了 if pktLen \u003c maxPacketSize { // zero allocations for non-split packets // 这个 if 表示只有一个小于最大长度的单包，都不要分片，直接读取完成返回 if prevData == nil { return data, nil } // 追加最后一个包到buf中，返回 return append(prevData, data...), nil } // 追加该包到buf中，继续读取剩余的数据 prevData = append(prevData, data...) } } // returns next N bytes from buffer. // The returned slice is only guaranteed to be valid until the next read func (b *buffer) readNext(need int) ([]byte, error) { if b.length \u003c need { // refill if err := b.fill(need); err != nil { return nil, err } } offset := b.idx b.idx += need b.length -= need return b.buf[offset:b.idx], nil } // fill reads into the buffer until at least _need_ bytes are in it func (b *buffer) fill(need int) error { n := b.length // fill data into its double-buffering target: if we've called // flip on this buffer, we'll be copying to the background buffer, // and then filling it with network data; otherwise we'll just move // the contents of the current buffer to the front before filling it dest := b.dbuf[b.flipcnt\u00261] // grow buffer if necessary to fit the whole packet. if need \u003e len(dest) { // Round up to the next multiple of the default size dest = make([]byte, ((need/defaultBufSize)+1)*defaultBufSize) // if the allocated buffer is not too large, move it to backing storage // to prevent extra allocations on applications that perform large reads if len(dest) \u003c= maxCachedBufSize { b.dbuf[b.flipcnt\u00261] = dest } } // if we're filling the fg buffer, move the existing data to the start of it. // if we're filling the bg buffer, copy over the data if n \u003e 0 { copy(dest[:n], b.buf[b.idx:]) } b.buf = dest b.idx = 0 for { if b.timeout \u003e 0 { if err := b.nc.SetReadDeadline(time.Now().Add(b.timeout)); err != nil { return err } } nn, err := b.nc.Read(b.buf[n:]) n += nn switch err { case nil: if n \u003c need { continue } b.length = n return nil case io.EOF: if n \u003e= need { b.length = n return nil } return io.ErrUnexpectedEOF default: return err } } } 写数据包到缓存区： // Write packet buffer 'data' func (mc *mysqlConn) writePacket(data []byte) error { // 真实数据长度 pktLen := len(data) - 4 // 大于最大允许的包长度 if pktLen \u003e mc.maxAllowedPacket { return ErrPktTooLarge } for { var size int // 该包已经超过单个包的大小上限 16MB 了 if pktLen \u003e= maxPacketSize { data[0] = 0xff data[1] = 0xff data[2] = 0xff size = maxPacketSize } else { data[0] = byte(pktLen) data[1] = byte(pktLen \u003e\u003e 8) data[2] = byte(pktLen \u003e\u003e 16) size = pktLen } data[3] = mc.sequence // Write packet // 写超时时间 if mc.writeTimeout \u003e 0 { if err := mc.netConn.","date":"2026-02-03","objectID":"/go_sql_drive_mysql/:2:2","tags":["GORM","Go"],"title":"mysql驱动","uri":"/go_sql_drive_mysql/"},{"categories":["Go语言"],"content":"基于 go1.22.6 版本解析 panic 的底层实现 在 go 语言中，一个 当前执行的 Goroutine 结构体中有一个 defer 链表的头指针和一个 panic 链表的头指针 每一个 panic 由一个 _panic 结构体表示，发生新的 panic 时，是在链表头部插入新的 _panic 结构体，链表头上的 _panic 就是当前正在执行那一个 // A _panic holds information about an active panic. // // A _panic value must only ever live on the stack. // // The argp and link fields are stack pointers, but don't need special // handling during stack growth: because they are pointer-typed and // _panic values only live on the stack, regular stack pointer // adjustment takes care of them. type _panic struct { argp unsafe.Pointer // pointer to arguments of deferred call run during panic; cannot move - known to liblink arg any // argument to panic link *_panic // link to earlier panic // startPC and startSP track where _panic.start was called. startPC uintptr startSP unsafe.Pointer // The current stack frame that we're running deferred calls for. sp unsafe.Pointer lr uintptr fp unsafe.Pointer // retpc stores the PC where the panic should jump back to, if the // function last returned by _panic.next() recovers the panic. retpc uintptr // Extra state for handling open-coded defers. deferBitsPtr *uint8 slotsPtr unsafe.Pointer recovered bool // whether this panic has been recovered goexit bool deferreturn bool } panic 关键字会被编译器转为调用 runtime.gopanic 函数，其实现如下： // The implementation of the predeclared function panic. // The compiler emits calls to this function. // // gopanic should be an internal detail, // but widely used packages access it using linkname. // Notable members of the hall of shame include: // - go.undefinedlabs.com/scopeagent // - github.com/goplus/igop // // Do not remove or change the type signature. // See go.dev/issue/67401. // //go:linkname gopanic func gopanic(e any) { if e == nil { if debug.panicnil.Load() != 1 { e = new(PanicNilError) } else { panicnil.IncNonDefault() } } gp := getg() if gp.m.curg != gp { print(\"panic: \") printpanicval(e) print(\"\\n\") throw(\"panic on system stack\") } if gp.m.mallocing != 0 { print(\"panic: \") printpanicval(e) print(\"\\n\") throw(\"panic during malloc\") } if gp.m.preemptoff != \"\" { print(\"panic: \") printpanicval(e) print(\"\\n\") print(\"preempt off reason: \") print(gp.m.preemptoff) print(\"\\n\") throw(\"panic during preemptoff\") } if gp.m.locks != 0 { print(\"panic: \") printpanicval(e) print(\"\\n\") throw(\"panic holding locks\") } var p _panic p.arg = e runningPanicDefers.Add(1) p.start(getcallerpc(), unsafe.Pointer(getcallersp())) for { fn, ok := p.nextDefer() if !ok { break } fn() } // If we're tracing, flush the current generation to make the trace more // readable. // // TODO(aktau): Handle a panic from within traceAdvance more gracefully. // Currently it would hang. Not handled now because it is very unlikely, and // already unrecoverable. if traceEnabled() { traceAdvance(false) } // ran out of deferred calls - old-school panic now // Because it is unsafe to call arbitrary user code after freezing // the world, we call preprintpanics to invoke all necessary Error // and String methods to prepare the panic strings before startpanic. preprintpanics(\u0026p) fatalpanic(\u0026p) // should not return *(*int)(nil) = 0 // not reached } ","date":"2026-02-03","objectID":"/panic/:0:0","tags":["Go"],"title":"Panic","uri":"/panic/"},{"categories":["Go语言"],"content":"从源码角度看 select 的底层实现 select 前言： 一个 go 代码，会经过编译器编译成一个可执行二进制程序，运行在不同的 cpu 架构上。 编译器编译 go 代码的过程包含以下过程，其中前三步又叫编译前端，后两步叫编译后端 词法分析器 –\u003e 语法分析器 –\u003e 类型检查 –\u003e 中间代码生成 –\u003e 机器码生成 1、词法分析器通过对每一行 go 代码，按照某个生产规则进行分割，组装成一个 Token，输入给语法分析器。例如：package, json, import, (, io, ), …，而语法分析会把 Token 序列转换成有意义的结构体，即语法树 2、类型检查会检查常量、类型、函数声明以及变量赋值语句的类型，src/cmd/compile/internal/typecheck文件夹下能看到具体的检查动作 3、类型检查同时还会对 go 的关键字进行替换改写，比如 Go 语言中很常见的内置函数 make，在类型检查阶段之前，无论是创建切片、哈希还是 Channel 用的都是 make 关键字，不过在类型检查阶段会根据创建的类型将 make 替换成特定的函数，后面生成中间代码的过程就不再会处理 OMAKE 类型的节点了，而是会依据生成的细分类型处理；编译器会先检查关键字 make 的第一个类型参数，根据类型的不同进入不同分支，切片分支 TSLICE、哈希分支 TMAP 和 Channel 分支 TCHAN 4、在生成中间代码之前，编译器还需要替换抽象语法树中节点的一些元素，这个替换的过程是通过 cmd/compile/internal/walk/expr.go 和cmd/compile/internal/walk/stmt.go 和相关函数实现的，这里简单展示几个函数的签名： func walk(fn *Node) func walkappend(n *Node, init *Nodes, dst *Node) *Node ... func walkrange(n *Node) *Node func walkselect(sel *Node) func walkselectcases(cases *Nodes) []*Node func walkstmt(n *Node) *Node func walkstmtlist(s []*Node) func walkswitch(sw *Node) 这些用于遍历抽象语法树的函数会将一些关键字和内建函数转换成函数调用，例如： 上述函数会将 panic、recover 两个内建函数转换成 runtime.gopanic 和 runtime.gorecover 两个真正运行时函数，而关键字 new 也会被转换成调用 runtime.newobject 函数。 5、生成机器码 这里主要了解大体流程，主要需要关注的是类型检查和中间码生成，在这里能够搞清 go 代码的入口是在哪里，一个内建函数是怎么由编译器跟运行时共同作用下实现的 ","date":"2026-02-03","objectID":"/select/:0:0","tags":["Go"],"title":"select","uri":"/select/"},{"categories":["Go语言"],"content":"1. 编译期间的 select select 语句在编译期间会被转换成 ir.OSELECT 类型的节点，见 src/cmd/compile/internal/walk/stmt.go 的 walkStmt() 函数： func walkStmt(n ir.Node) ir.Node { ... case ir.OSELECT: n := n.(*ir.SelectStmt) walkSelect(n) return n case ir.OSWITCH: n := n.(*ir.SwitchStmt) walkSwitch(n) return n case ir.ORANGE: n := n.(*ir.RangeStmt) return walkRange(n) ... } 交由，go/src/cmd/compile/internal/walk/select.go 实现： func walkSelect(sel *ir.SelectStmt) { lno := ir.SetPos(sel) if sel.Walked() { base.Fatalf(\"double walkSelect\") } sel.SetWalked(true) init := ir.TakeInit(sel) init = append(init, walkSelectCases(sel.Cases)...) sel.Cases = nil sel.Compiled = init walkStmtList(sel.Compiled) base.Pos = lno } 在 walkSelect 中主要逻辑在 walkSelectCases 中，在 walkSelectCases中根据对不同case分支条件的处理，不同的情况会调用不同的运行时函数，大致分为如下四类： 根据walkSelectCases函数的具体代码来看： ","date":"2026-02-03","objectID":"/select/:1:0","tags":["Go"],"title":"select","uri":"/select/"},{"categories":["Go语言"],"content":"1.1. case1：select 没有 case 分支 表现形式就是： select{} func walkSelectCases(cases []*ir.CommClause) []ir.Node { ncas := len(cases) // case分支的数目 sellineno := base.Pos // optimization: zero-case select if ncas == 0 { return []ir.Node{mkcallstmt(\"block\")} } } 调用 runtime.block()，位于go/src/runtime/select.go func block() { gopark(nil, nil, waitReasonSelectNoCases, traceBlockForever, 1) // forever } 会直接报死锁，不会得到执行 ","date":"2026-02-03","objectID":"/select/:1:1","tags":["Go"],"title":"select","uri":"/select/"},{"categories":["Go语言"],"content":"1.2. case2：select 只有一个非 default case select 只有非 default 的分支时，实际会被编译器转换为对 channel 的读写操作，实际调用 data := \u003c- ch 或 ch \u003c- data 并没有什么区别 ch := make(chan struct{}) select { case data \u003c- ch: fmt.Printf(\"ch data: %v\\n\", data) } 会被编译器转换为： data := \u003c- ch fmt.Printf(\"ch data: %v\\n\", data) func walkSelectCases(cases []*ir.CommClause) []ir.Node { ... // optimization: one-case select: single op. if ncas == 1 { cas := cases[0] // 只有一个case ir.SetPos(cas) l := cas.Init() if cas.Comm != nil { // not default: n := cas.Comm // 获取case的条件语句 l = append(l, ir.TakeInit(n)...) switch n.Op() { default: base.Fatalf(\"select %v\", n.Op()) case ir.OSEND: // 如果是往chan里写，即 ch \u003c- data，不需要编译器做转换， // already ok case ir.OSELRECV2: // 从chan中接收 r := n.(*ir.AssignListStmt) // 接收操作完整形式：d,ok := \u003c- ch if ir.IsBlank(r.Lhs[0]) \u0026\u0026 ir.IsBlank(r.Lhs[1]) { // 如果不关心返回值，直接转成 \u003c- ch n = r.Rhs[0] break } r.SetOp(ir.OAS2RECV) // 否则，是 data, ok := \u003c- chan 这种形式 } l = append(l, n) // 把编译器处理后的case语句条件加入待执行语句列表 } l = append(l, cas.Body...) // 把case条件后要执行的语句体加入待执行语句列表 l = append(l, ir.NewBranchStmt(base.Pos, ir.OBREAK, nil)) // 默认加入break类型语句，跳出select-case语句体 return l } ... } 这是一种编译器在语法树级别对 select 语句的静态优化，可以有效简化运行时的开销。 ","date":"2026-02-03","objectID":"/select/:1:2","tags":["Go"],"title":"select","uri":"/select/"},{"categories":["Go语言"],"content":"1.3. case3：select 有一个 case 和一个 default ch := make(chan int) select { case ch \u003c- 1: fmt.Println(\"run case 1\") default: fmt.Println(\"run default\") } 会被编译器替换为： if selectnbsend(ch, 1) { fmt.Println(\"run case 1\") } else { fmt.Println(\"run default\") } 看 walkSelectCases 的后续分支： func walkSelectCases(cases []*ir.CommClause) []ir.Node { ... // optimization: two-case select but one is default: single non-blocking op. if ncas == 2 \u0026\u0026 dflt != nil { cas := cases[0] if cas == dflt { cas = cases[1] } n := cas.Comm ir.SetPos(n) r := ir.NewIfStmt(base.Pos, nil, nil, nil) // 创建一个新的 if 语句 r，用于表示优化后的代码结构。 r.SetInit(cas.Init()) // 将该 case 的初始化语句设置为 if 语句的初始化部分。 var cond ir.Node switch n.Op() { default: base.Fatalf(\"select %v\", n.Op()) case ir.OSEND: // 往chan里发送数据 // if selectnbsend(c, v) { body } else { default body } n := n.(*ir.SendStmt) ch := n.Chan cond = mkcall1(chanfn(\"selectnbsend\", 2, ch.Type()), types.Types[types.TBOOL], r.PtrInit(), ch, n.Value) case ir.OSELRECV2: // 从chan里接收数据 n := n.(*ir.AssignListStmt) recv := n.Rhs[0].(*ir.UnaryExpr) ch := recv.X elem := n.Lhs[0] if ir.IsBlank(elem) { elem = typecheck.NodNil() } cond = typecheck.TempAt(base.Pos, ir.CurFunc, types.Types[types.TBOOL]) fn := chanfn(\"selectnbrecv\", 2, ch.Type()) call := mkcall1(fn, fn.Type().ResultsTuple(), r.PtrInit(), elem, ch) as := ir.NewAssignListStmt(r.Pos(), ir.OAS2, []ir.Node{cond, n.Lhs[1]}, []ir.Node{call}) r.PtrInit().Append(typecheck.Stmt(as)) } r.Cond = typecheck.Expr(cond) // 将非阻塞操作的结果布尔值 cond 作为 if 语句的条件。 r.Body = cas.Body // 设置 if 的主体为非 default 分支的执行体 // 将 default 分支的初始化语句和执行体作为 else 分支。 r.Else = append(dflt.Init(), dflt.Body...) // 构造一个包含 if 语句和一个 break 语句的语法树节点列表，break 语句用于跳出 select 的控制流 return []ir.Node{r, ir.NewBranchStmt(base.Pos, ir.OBREAK, nil)} } ... } 编译器会将其转为调用运行时 runtime.selectnbrecv() 函数和runtime.selectnbsend()函数，这两个函数会分别调用runtime.cahnrecv()函数和runtime.chansend()函数，传入这两个函数的 block 参数为 false，代表非阻塞，不成功则直接返回。 ","date":"2026-02-03","objectID":"/select/:1:3","tags":["Go"],"title":"select","uri":"/select/"},{"categories":["Go语言"],"content":"1.4. case4：select 有多个可读写的 case ch1 := make(chan int) ch2 := make(chan int) select { case ch1 \u003c- 1: fmt.Println(\"run case 1\") case data := \u003c- ch2: fmt.Printf(\"run case 2, data is: %d\", data) } func walkSelectCases(cases []*ir.CommClause) []ir.Node { ... if dflt != nil { // 过滤掉default分支，case数目减1，以下逻辑只对非default多case分支选择，若多chan分支都没准备好，走default分支 ncas-- } casorder := make([]*ir.CommClause, ncas) // casorder为ncas大小的case语句的数组 nsends, nrecvs := 0, 0 // 对接收操作和发送操作分类 var init []ir.Node // 定义init为多case编译后待执行的语句列表 // generate sel-struct base.Pos = sellineno // 通过scasetype()构造长度同ncas的scase结构体数组 selv := typecheck.TempAt(base.Pos, ir.CurFunc, types.NewArray(scasetype(), int64(ncas))) init = append(init, typecheck.Stmt(ir.NewAssignStmt(base.Pos, selv, nil))) // No initialization for order; runtime.selectgo is responsible for that. // 定义order为2倍的ncas长度的TUINT16类型的数组 // 后续这两者，selv和order作为runtime.selectgo()函数的入参，前者存放scase列表内存地址，后者用来做scase排序使用，排序是为了便于挑选出待执行的case order := typecheck.TempAt(base.Pos, ir.CurFunc, types.NewArray(types.Types[types.TUINT16], 2*int64(ncas))) var pc0, pcs ir.Node if base.Flag.Race { // 竞态分析 pcs = typecheck.TempAt(base.Pos, ir.CurFunc, types.NewArray(types.Types[types.TUINTPTR], int64(ncas))) pc0 = typecheck.Expr(typecheck.NodAddr(ir.NewIndexExpr(base.Pos, pcs, ir.NewInt(base.Pos, 0)))) } else { pc0 = typecheck.NodNil() } // register cases 开始注册case，遍历生成scase对象放到selv中 for _, cas := range cases { ir.SetPos(cas) init = append(init, ir.TakeInit(cas)...) n := cas.Comm if n == nil { // default:跳过 continue } var i int var c, elem ir.Node switch n.Op() { default: base.Fatalf(\"select %v\", n.Op()) case ir.OSEND: n := n.(*ir.SendStmt) i = nsends // 对发送类型的chan，i 从 0 开始往 ncas 计数 nsends++ c = n.Chan elem = n.Value case ir.OSELRECV2: n := n.(*ir.AssignListStmt) nrecvs++ i = ncas - nrecvs // 对接收类型的chann，i 从 ncas 往前计数 recv := n.Rhs[0].(*ir.UnaryExpr) c = recv.X elem = n.Lhs[0] // data,ok := \u003c- ch } casorder[i] = cas // 编译器对多个case排列后，发送chan的case在左边，接收chan的case在右边，在selv中也是如此 // 定义一个函数，写入chan或elem到selv数组, 实际执行的是selv[i]=f setField := func(f string, val ir.Node) { r := ir.NewAssignStmt(base.Pos, ir.NewSelectorExpr(base.Pos, ir.ODOT, ir.NewIndexExpr(base.Pos, selv, ir.NewInt(base.Pos, int64(i))), typecheck.Lookup(f)), val) init = append(init, typecheck.Stmt(r)) } // 将c指针转换为 unsafe.Pointer c = typecheck.ConvNop(c, types.Types[types.TUNSAFEPTR]) // 写入 c 代表的 chan 到 selv，selv[c]=c setField(\"c\", c) if !ir.IsBlank(elem) { elem = typecheck.ConvNop(elem, types.Types[types.TUNSAFEPTR]) // 写入 elem 到 selv，selv[elem]=elem setField(\"elem\", elem) } // TODO(mdempsky): There should be a cleaner way to // handle this. if base.Flag.Race { r := mkcallstmt(\"selectsetpc\", typecheck.NodAddr(ir.NewIndexExpr(base.Pos, pcs, ir.NewInt(base.Pos, int64(i))))) init = append(init, r) } } // 发送与接收chan数目与总数目不对，panic if nsends+nrecvs != ncas { base.Fatalf(\"walkSelectCases: miscount: %v + %v != %v\", nsends, nrecvs, ncas) } // run the select base.Pos = sellineno chosen := typecheck.TempAt(base.Pos, ir.CurFunc, types.Types[types.TINT]) recvOK := typecheck.TempAt(base.Pos, ir.CurFunc, types.Types[types.TBOOL]) r := ir.NewAssignListStmt(base.Pos, ir.OAS2, nil, nil) r.Lhs = []ir.Node{chosen, recvOK} // 左边有双值，一个chosen，一个recvOK，分别代表选择了哪个case分支，是否接收成功 fn := typecheck.LookupRuntime(\"selectgo\") // 调用 runtime.selectgo var fnInit ir.Nodes // r右边调用runtime.selectgo函数，传递 selv，order，nsend，nrecvs，block是否阻塞等参数 r.Rhs = []ir.Node{mkcall1(fn, fn.Type().ResultsTuple(), \u0026fnInit, bytePtrToIndex(selv, 0), bytePtrToIndex(order, 0), pc0, ir.NewInt(base.Pos, int64(nsends)), ir.NewInt(base.Pos, int64(nrecvs)), ir.NewBool(base.Pos, dflt == nil))} init = append(init, fnInit...) init = append(init, typecheck.Stmt(r)) // selv, order, and pcs (if race) are no longer alive after selectgo. // dispatch cases 分发case，定义一个函数，根据chosen确定的case分支生成if语句，执行该分支的语句 dispatch := func(cond ir.Node, cas *ir.CommClause) { var list ir.Nodes // 一个 ir.Nodes 类型的切片，用于存储该分支需要执行的所有语句 if n := cas.Comm; n != nil \u0026\u0026 n.Op() == ir.OSELRECV2 { ","date":"2026-02-03","objectID":"/select/:1:4","tags":["Go"],"title":"select","uri":"/select/"},{"categories":["Go语言"],"content":"2. 运行时的 select ","date":"2026-02-03","objectID":"/select/:2:0","tags":["Go"],"title":"select","uri":"/select/"},{"categories":["Go语言"],"content":"2.1. runtime.selectgo 实现 // selectgo implements the select statement. // // cas0 points to an array of type [ncases]scase, and order0 points to // an array of type [2*ncases]uint16 where ncases must be \u003c= 65536. // Both reside on the goroutine's stack (regardless of any escaping in // selectgo). // // For race detector builds, pc0 points to an array of type // [ncases]uintptr (also on the stack); for other builds, it's set to // nil. // // selectgo returns the index of the chosen scase, which matches the // ordinal position of its respective select{recv,send,default} call. // Also, if the chosen scase was a receive operation, it reports whether // a value was received. func selectgo(cas0 *scase, order0 *uint16, pc0 *uintptr, nsends, nrecvs int, block bool) (int, bool) { ... // NOTE: In order to maintain a lean stack size, the number of scases // is capped at 65536. cas1 := (*[1 \u003c\u003c 16]scase)(unsafe.Pointer(cas0)) // 64kb cas1,128kb order1 order1 := (*[1 \u003c\u003c 17]uint16)(unsafe.Pointer(order0)) ncases := nsends + nrecvs // ncases个数是发送chan个数nsends加上接收chan个数nrecvs scases := cas1[:ncases:ncases] // scases切片是上面分配cas1数组的前ncases个元素,并且数组容量也为ncases pollorder := order1[:ncases:ncases] // pollorder是order1数组的前ncases个元素，与lockorder共用一个底层数组，避免多次分配 lockorder := order1[ncases:][:ncases:ncases] //加锁列表lockorder是order1数组的第二批ncase个元素 norder := 0 for i := range scases { cas := \u0026scases[i] // Omit cases without channels from the poll and lock orders. if cas.c == nil { cas.elem = nil // allow GC continue } j := cheaprandn(uint32(norder + 1)) pollorder[norder] = pollorder[j] pollorder[j] = uint16(i) norder++ } pollorder = pollorder[:norder] lockorder = lockorder[:norder] // sort the cases by Hchan address to get the locking order. // simple heap sort, to guarantee n log n time and constant stack footprint. for i := range lockorder { j := i // Start with the pollorder to permute cases on the same channel. c := scases[pollorder[i]].c for j \u003e 0 \u0026\u0026 scases[lockorder[(j-1)/2]].c.sortkey() \u003c c.sortkey() { k := (j - 1) / 2 lockorder[j] = lockorder[k] j = k } lockorder[j] = pollorder[i] } for i := len(lockorder) - 1; i \u003e= 0; i-- { o := lockorder[i] c := scases[o].c lockorder[i] = lockorder[0] j := 0 for { k := j*2 + 1 if k \u003e= i { break } if k+1 \u003c i \u0026\u0026 scases[lockorder[k]].c.sortkey() \u003c scases[lockorder[k+1]].c.sortkey() { k++ } if c.sortkey() \u003c scases[lockorder[k]].c.sortkey() { lockorder[j] = lockorder[k] j = k continue } break } lockorder[j] = o } // lock all the channels involved in the select sellock(scases, lockorder) ... } 2.1.1. 初始化 1）pollorder 和 lockorder，用于存储 select 语句中通道的随机顺序和锁定顺序；初始化 lockorder 和 pollorder，采用随机数生成器将随机化后的 case 索引存储在 pollorder 中，可以避免 channel 的饥饿问题，保证公平性，后续遍历该数组进而遍历所有 case 分支 2）构建一个最大堆，通过堆排序下沉的操作，每次调整堆结构，保证按照通道的地址值从大到小排序，并将对应的 scase 数组的下标索引存在 lockorder 中。 3）调用sellock依次对lockorder中的所有 case 的 chan 进行上锁，避免并发时发生死锁。这也是为什么要顺序上锁的原因 // 加锁，正序 func sellock(scases []scase, lockorder []uint16) { var c *hchan for _, o := range lockorder { c0 := scases[o].c // 跳过重复加锁，并更新c为前一个 if c0 != c { c = c0 lock(\u0026c.lock) } } } // 解锁，倒叙 func selunlock(scases []scase, lockorder []uint16) { for i := len(lockorder) - 1; i \u003e= 0; i-- { c := scases[lockorder[i]].c if i \u003e 0 \u0026\u0026 c == scases[lockorder[i-1]].c { continue } unlock(\u0026c.lock) } } 2.1.2. 第一阶段处理 4）开始第一轮查询 func selectgo(cas0 *scase, order0 *uint16, pc0 *uintptr, nsends, nrecvs int, block bool) (int, bool) { ... var ( gp *g sg *sudog c *hchan k *scase sglist *sudog sgnext *sudog qp unsafe.Pointer nextp **sudog ) // pass 1 - look for something already waiting var casi int var cas *scase var caseSuccess bool var caseReleaseTime int64 = -1 var recvOK bool for _, casei := range pollorder { casi = int(casei) // case的索引 cas = \u0026scases[casi] // 当前case c = cas.c // 因为从编译器保证了传进来的scase数组是按照send在前，recv在后的顺序，并且不包含default分支 if casi \u003e= nsends { // 所以说明该case是接收chan，另外下面这几个if的前后顺序也有讲究 sg = c.sendq.dequeue() // 在该chan的发送队列有阻塞等待发送的g，则goto到recv从缓冲区读取数据后将等待goroutine中的数据放入到缓冲区中相同的位置 if sg != nil { goto recv } if c.qcount \u003e 0 { // 当前chan是有缓冲ch","date":"2026-02-03","objectID":"/select/:2:1","tags":["Go"],"title":"select","uri":"/select/"},{"categories":["生活"],"content":"开始写博客 今天终于搭建好了自己的博客，使用 Hugo + LoveIt 主题，感觉还不错！ ","date":"2026-02-03","objectID":"/first_post/:1:0","tags":["随笔","开始"],"title":"我的第一篇博客","uri":"/first_post/"},{"categories":["生活"],"content":"为什么要写博客？ 写博客有很多好处： 记录学习过程 - 把学到的知识整理成文章，加深理解 分享经验 - 帮助遇到同样问题的人 提升表达能力 - 锻炼文字组织和表达能力 建立个人品牌 - 在技术社区建立自己的影响力 ","date":"2026-02-03","objectID":"/first_post/:1:1","tags":["随笔","开始"],"title":"我的第一篇博客","uri":"/first_post/"},{"categories":["生活"],"content":"接下来的计划 📝 定期更新技术文章 💡 分享项目经验和踩坑记录 🎯 记录个人成长和思考 🚀 持续学习新技术 ","date":"2026-02-03","objectID":"/first_post/:1:2","tags":["随笔","开始"],"title":"我的第一篇博客","uri":"/first_post/"},{"categories":["生活"],"content":"一些想法 “The best time to plant a tree was 20 years ago. The second best time is now.” 种一棵树最好的时间是十年前，其次是现在。 开始永远不晚，重要的是迈出第一步。希望能坚持写下去，记录自己的技术成长之路。 Stay hungry, stay foolish! 🚀 ","date":"2026-02-03","objectID":"/first_post/:1:3","tags":["随笔","开始"],"title":"我的第一篇博客","uri":"/first_post/"}]